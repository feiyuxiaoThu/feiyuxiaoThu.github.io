<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Tendourisu 的个人网站"><meta name=author content=Tendourisu><link href=https://tendourisu.github.io/notes/easy-rl/%E7%8E%8B%E6%A0%91%E6%A3%AEReinforcement_Learning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ rel=canonical><link href=../RLfromDeepSeek/ rel=prev><link href=../../cs231n/cs231n/ rel=next><link rel=icon href=../../../img/aris.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.9"><title>王树森Reinforcement_Learning学习笔记 - Tendourisu's Site</title><link rel=stylesheet href=../../../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=JetBrains+Mono:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"JetBrains Mono";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../css/custom.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=../../../css/fold_toc.css><link rel=stylesheet href=../../../css/card.css><link rel=stylesheet href=../../../css/flink.css><link rel=stylesheet href=../../../css/tasklist.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#_1 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="Tendourisu's Site" class="md-header__button md-logo" aria-label="Tendourisu's Site" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Tendourisu's Site </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 王树森Reinforcement_Learning学习笔记 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=indigo aria-label="light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="dark mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/Tendourisu/tendourisu.github.io/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Tendourisu's Site </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"/></svg> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"/></svg> Notes </a> </li> <li class=md-tabs__item> <a href=../../../blogs/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48z"/></svg> Blogs </a> </li> <li class=md-tabs__item> <a href=../../../summary/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"/></svg> Summaries </a> </li> <li class=md-tabs__item> <a href=../../../Tools/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16h-2v-1H8v1H6v-1H2v5h20v-5h-4zm2-8h-3V6c0-1.1-.9-2-2-2H9c-1.1 0-2 .9-2 2v2H4c-1.1 0-2 .9-2 2v4h4v-2h2v2h8v-2h2v2h4v-4c0-1.1-.9-2-2-2m-5 0H9V6h6z"/></svg> Tools </a> </li> <li class=md-tabs__item> <a href=../../../tags/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5.5 7A1.5 1.5 0 0 1 4 5.5 1.5 1.5 0 0 1 5.5 4 1.5 1.5 0 0 1 7 5.5 1.5 1.5 0 0 1 5.5 7m15.91 4.58-9-9C12.05 2.22 11.55 2 11 2H4c-1.11 0-2 .89-2 2v7c0 .55.22 1.05.59 1.41l8.99 9c.37.36.87.59 1.42.59s1.05-.23 1.41-.59l7-7c.37-.36.59-.86.59-1.41 0-.56-.23-1.06-.59-1.42"/></svg> Tags </a> </li> <li class=md-tabs__item> <a href=../../../links/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.78 3.653a3.936 3.936 0 1 1 5.567 5.567l-3.627 3.627a3.936 3.936 0 0 1-5.88-.353.75.75 0 0 0-1.18.928 5.436 5.436 0 0 0 8.12.486l3.628-3.628a5.436 5.436 0 1 0-7.688-7.688l-3 3a.75.75 0 0 0 1.06 1.061z"/><path d="M7.28 11.153a3.936 3.936 0 0 1 5.88.353.75.75 0 0 0 1.18-.928 5.436 5.436 0 0 0-8.12-.486L2.592 13.72a5.436 5.436 0 1 0 7.688 7.688l3-3a.75.75 0 1 0-1.06-1.06l-3 3a3.936 3.936 0 0 1-5.567-5.568z"/></svg> Links </a> </li> <li class=md-tabs__item> <a href=../../../about/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-91.9-186.5C182 346.2 212.6 368 256 368s74-21.8 91.9-42.5c5.8-6.7 15.9-7.4 22.6-1.6s7.4 15.9 1.6 22.6c-22.3 25.6-61 53.5-116.1 53.5s-93.8-27.9-116.1-53.5c-5.8-6.7-5.1-16.8 1.6-22.6s16.8-5.1 22.6 1.6M144.4 208a32 32 0 1 1 64 0 32 32 0 1 1-64 0m156.4 25.6c-5.3 7.1-15.3 8.5-22.4 3.2s-8.5-15.3-3.2-22.4c30.4-40.5 91.2-40.5 121.6 0 5.3 7.1 3.9 17.1-3.2 22.4s-17.1 3.9-22.4-3.2c-17.6-23.5-52.8-23.5-70.4 0"/></svg> About </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="Tendourisu's Site" class="md-nav__button md-logo" aria-label="Tendourisu's Site" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"/></svg> </a> Tendourisu's Site </label> <div class=md-nav__source> <a href=https://github.com/Tendourisu/tendourisu.github.io/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Tendourisu's Site </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"/></svg> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"/></svg> <span class=md-ellipsis> Notes </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> CS61C </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> CS61C </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../cs61c/lec01-Intro/ class=md-nav__link> <span class=md-ellipsis> lec01-Intro </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec02-C%E5%9F%BA%E7%A1%801/ class=md-nav__link> <span class=md-ellipsis> lec02-C基础1 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec03-C%E5%9F%BA%E7%A1%802/ class=md-nav__link> <span class=md-ellipsis> lec03-C基础2 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec04-C%E5%9F%BA%E7%A1%803/ class=md-nav__link> <span class=md-ellipsis> lec04-C基础3 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec05-C%E6%B5%AE%E7%82%B9%E6%95%B0%E5%9F%BA%E7%A1%80/ class=md-nav__link> <span class=md-ellipsis> lec05-C浮点数基础 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec06-RISCV%E6%8C%87%E4%BB%A4%E5%9F%BA%E7%A1%801/ class=md-nav__link> <span class=md-ellipsis> lec06-RISCV指令基础1 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec07-RISCV%E6%8C%87%E4%BB%A4%E5%9F%BA%E7%A1%802/ class=md-nav__link> <span class=md-ellipsis> lec07-RISCV指令基础2 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec08-RISCV%E6%8C%87%E4%BB%A4%E6%80%BB%E7%BB%93/ class=md-nav__link> <span class=md-ellipsis> lec08-RISCV指令总结 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec09-RISC-V%E6%B1%87%E7%BC%96-%E7%BC%96%E8%AF%91-%E9%93%BE%E6%8E%A5%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93/ class=md-nav__link> <span class=md-ellipsis> lec09-lec09-RISC-V汇编-编译-链接流程总结 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec10-%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF%E5%9F%BA%E7%A1%80/ class=md-nav__link> <span class=md-ellipsis> lec10-数字电路基础 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec11-%E6%97%B6%E5%BA%8F%E7%94%B5%E8%B7%AF%26%E6%A8%A1%E6%8B%9F%E7%94%B5%E8%B7%AF%E5%9F%BA%E7%A1%80/ class=md-nav__link> <span class=md-ellipsis> lec11-时序电路&模拟电路基础 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec12-RISC-V%E5%A4%84%E7%90%86%E5%99%A8%E8%AE%BE%E8%AE%A1/ class=md-nav__link> <span class=md-ellipsis> lec12-RISC-V处理器设计 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec13-RISC-V%E7%9A%84%E6%8E%A7%E5%88%B6%E5%99%A8%E5%AE%9E%E7%8E%B0%E4%B8%8E%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%85%A5%E9%97%A8/ class=md-nav__link> <span class=md-ellipsis> lec13-RISC-V的控制器实现与流水线入门 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec14-RISC-V%20%E6%B5%81%E6%B0%B4%E7%BA%BF%E4%B8%8E%E5%A4%84%E7%90%86%E5%99%A8/ class=md-nav__link> <span class=md-ellipsis> lec14-RISC-V 流水线与处理器 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec15-%E7%BC%93%E5%AD%98%E7%B3%BB%E7%BB%9F/ class=md-nav__link> <span class=md-ellipsis> lec15-缓存系统 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec16-%E7%BC%93%E5%AD%98%E7%B3%BB%E7%BB%9F%E6%80%BB%E7%BB%93/ class=md-nav__link> <span class=md-ellipsis> lec16-缓存系统总结 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/lec17-%E7%BC%93%E5%AD%98%E6%80%BB%E7%BB%93/ class=md-nav__link> <span class=md-ellipsis> lec17-缓存总结 </span> </a> </li> <li class=md-nav__item> <a href=../../cs61c/riscv%E4%BB%A3%E7%A0%81%E4%B9%A0%E6%83%AF/ class=md-nav__link> <span class=md-ellipsis> riscv代码习惯 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3 checked> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> easy-rl </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=true> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> easy-rl </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../easy-rl-chapter1-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter1-强化学习基础 </span> </a> </li> <li class=md-nav__item> <a href=../easy-rl-chapter2-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8BMDP/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter2-马尔科夫决策过程MDP </span> </a> </li> <li class=md-nav__item> <a href=../easy-rl-chapter3-%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%96%B9%E6%B3%95/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter3-表格型方法 </span> </a> </li> <li class=md-nav__item> <a href=../easy-rl-chapter4-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter4-策略梯度 </span> </a> </li> <li class=md-nav__item> <a href=../easy-rl-chapter5-%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96PPO/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter5-近端策略优化PPO </span> </a> </li> <li class=md-nav__item> <a href=../easy-rl-chapter6-DQN/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter6-DQN </span> </a> </li> <li class=md-nav__item> <a href=../easy-rl-chapter7-DQN%E8%BF%9B%E9%98%B6%E6%8A%80%E5%B7%A7/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter7-DQN进阶技巧 </span> </a> </li> <li class=md-nav__item> <a href=../easy-rl-chapter8-%E9%92%88%E5%AF%B9%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter8-针对连续动作的深度Q网络 </span> </a> </li> <li class=md-nav__item> <a href=../easy-rl-chapter9-%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%91%98%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/ class=md-nav__link> <span class=md-ellipsis> easy-rl-chapter9-演员-评论员算法总结 </span> </a> </li> <li class=md-nav__item> <a href=../POMDP%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E/ class=md-nav__link> <span class=md-ellipsis> POMDP的一些参数说明 </span> </a> </li> <li class=md-nav__item> <a href=../RL%E5%85%A5%E9%97%A8%E6%B1%87%E6%80%BB/ class=md-nav__link> <span class=md-ellipsis> RL入门汇总 </span> </a> </li> <li class=md-nav__item> <a href=../RLfromDeepSeek/ class=md-nav__link> <span class=md-ellipsis> RLfromDeepSeek </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 王树森Reinforcement_Learning学习笔记 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 王树森Reinforcement_Learning学习笔记 </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 【课程目录】 </span> </a> </li> <li class=md-nav__item> <a href=#p1 class=md-nav__link> <span class=md-ellipsis> P1 强化学习基础 </span> </a> <nav class=md-nav aria-label="P1 强化学习基础"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#terminologies class=md-nav__link> <span class=md-ellipsis> 一、名词（Terminologies） </span> </a> </li> <li class=md-nav__item> <a href=#return-and-value class=md-nav__link> <span class=md-ellipsis> 二、Return and Value </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p2-value-based-reinforcement-learning class=md-nav__link> <span class=md-ellipsis> P2 价值学习 (Value-Based Reinforcement Learning) </span> </a> <nav class=md-nav aria-label="P2 价值学习 (Value-Based Reinforcement Learning)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#deep-q-networkdqn class=md-nav__link> <span class=md-ellipsis> 一、Deep Q-Network(DQN) </span> </a> </li> <li class=md-nav__item> <a href=#temporal-difference-learning class=md-nav__link> <span class=md-ellipsis> 二、时间差分算法(Temporal Difference Learning) </span> </a> </li> <li class=md-nav__item> <a href=#td-dqntd-learning-for-dqn class=md-nav__link> <span class=md-ellipsis> 三、用 TD 算法训练 DQN(TD Learning for DQN) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p3-policy-based-reinforcement-learning class=md-nav__link> <span class=md-ellipsis> P3 策略学习 (Policy-Based Reinforcement Learning) </span> </a> <nav class=md-nav aria-label="P3 策略学习 (Policy-Based Reinforcement Learning)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 一、策略函数近似 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 二、策略学习的目标函数 </span> </a> </li> <li class=md-nav__item> <a href=#policy-gradient class=md-nav__link> <span class=md-ellipsis> 三、策略梯度(Policy Gradient) </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 四、使用策略梯度更新策略网络 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p4-actor-critic-methods class=md-nav__link> <span class=md-ellipsis> P4 运动员-裁判算法 (Actor-Critic Methods) </span> </a> <nav class=md-nav aria-label="P4 运动员-裁判算法 (Actor-Critic Methods)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 一、价值函数近似 </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 二、训练流程 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p5-alphago class=md-nav__link> <span class=md-ellipsis> P5 AlphaGo </span> </a> <nav class=md-nav aria-label="P5 AlphaGo"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#alphago class=md-nav__link> <span class=md-ellipsis> 一、AlphaGo设计思路 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> 二、蒙特卡洛树搜索主要思想 </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> 三、蒙特卡洛树搜索四步骤 </span> </a> </li> <li class=md-nav__item> <a href=#alphago-zero-alphago class=md-nav__link> <span class=md-ellipsis> 四、AlphaGo Zero 与 AlphaGo </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p6 class=md-nav__link> <span class=md-ellipsis> P6 蒙特卡罗方法 </span> </a> <nav class=md-nav aria-label="P6 蒙特卡罗方法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_9 class=md-nav__link> <span class=md-ellipsis> 一、 蒙特卡罗方法算法简介 </span> </a> </li> <li class=md-nav__item> <a href=#_10 class=md-nav__link> <span class=md-ellipsis> 二、时间差分学习与蒙特卡罗方法对比 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p7-fisher-yates class=md-nav__link> <span class=md-ellipsis> P7 随机排列与Fisher-Yates算法 </span> </a> <nav class=md-nav aria-label="P7 随机排列与Fisher-Yates算法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_11 class=md-nav__link> <span class=md-ellipsis> 一、随机排列问题 </span> </a> </li> <li class=md-nav__item> <a href=#fisher-yates class=md-nav__link> <span class=md-ellipsis> 二、Fisher-Yates算法 </span> </a> </li> <li class=md-nav__item> <a href=#fisher-yates_1 class=md-nav__link> <span class=md-ellipsis> 三、改进版Fisher-Yates算法过程 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p8-sarsa class=md-nav__link> <span class=md-ellipsis> P8 Sarsa算法 </span> </a> <nav class=md-nav aria-label="P8 Sarsa算法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sarsatabular-version class=md-nav__link> <span class=md-ellipsis> 一、Sarsa(Tabular Version) </span> </a> </li> <li class=md-nav__item> <a href=#sarsavalue-network-version class=md-nav__link> <span class=md-ellipsis> 二、Sarsa(Value Network Version) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p9-q-learning class=md-nav__link> <span class=md-ellipsis> P9 Q-Learning算法 </span> </a> <nav class=md-nav aria-label="P9 Q-Learning算法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#q-learningtabular-version class=md-nav__link> <span class=md-ellipsis> 一、Q-Learning(Tabular Version) </span> </a> </li> <li class=md-nav__item> <a href=#q-learningdqn-version class=md-nav__link> <span class=md-ellipsis> 二、Q-Learning(DQN Version) </span> </a> </li> <li class=md-nav__item> <a href=#sarsa-q-learning class=md-nav__link> <span class=md-ellipsis> 三、Sarsa 与 Q-Learning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p10-multi-step-td-target class=md-nav__link> <span class=md-ellipsis> P10 Multi-Step TD Target </span> </a> </li> <li class=md-nav__item> <a href=#p11-experience-replay class=md-nav__link> <span class=md-ellipsis> P11 经验回放 Experience Replay </span> </a> <nav class=md-nav aria-label="P11 经验回放 Experience Replay"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#experience-replay class=md-nav__link> <span class=md-ellipsis> 一、Experience Replay </span> </a> </li> <li class=md-nav__item> <a href=#prioritized-experience-replay class=md-nav__link> <span class=md-ellipsis> 二、Prioritized Experience Replay </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p12-target-networkdouble-d class=md-nav__link> <span class=md-ellipsis> P12 高估问题、Target Network、Double D </span> </a> <nav class=md-nav aria-label="P12 高估问题、Target Network、Double D"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_12 class=md-nav__link> <span class=md-ellipsis> 一、高估问题 </span> </a> </li> <li class=md-nav__item> <a href=#target-network class=md-nav__link> <span class=md-ellipsis> 二、Target Network </span> </a> </li> <li class=md-nav__item> <a href=#double-d class=md-nav__link> <span class=md-ellipsis> 三、Double D </span> </a> </li> <li class=md-nav__item> <a href=#td-target class=md-nav__link> <span class=md-ellipsis> 四、三种TD target计算方式对比 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> CS231N </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> CS231N </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../cs231n/cs231n/ class=md-nav__link> <span class=md-ellipsis> cs231n </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../../../blogs/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48z"/></svg> <span class=md-ellipsis> Blogs </span> </a> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Blogs </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../../../summary/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"/></svg> <span class=md-ellipsis> Summaries </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Summaries </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> 2025 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> 2025 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../summary/2025/2025-W08-02/ class=md-nav__link> <span class=md-ellipsis> 2025-W08-02 </span> </a> </li> <li class=md-nav__item> <a href=../../../summary/2025/2025-W09-02/ class=md-nav__link> <span class=md-ellipsis> 2025-W09-02 </span> </a> </li> <li class=md-nav__item> <a href=../../../summary/2025/2025-W11-03/ class=md-nav__link> <span class=md-ellipsis> 2025-W11-03 </span> </a> </li> <li class=md-nav__item> <a href=../../../summary/2025/2025-W12-03/ class=md-nav__link> <span class=md-ellipsis> 2025-W12-03 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../../../Tools/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16h-2v-1H8v1H6v-1H2v5h20v-5h-4zm2-8h-3V6c0-1.1-.9-2-2-2H9c-1.1 0-2 .9-2 2v2H4c-1.1 0-2 .9-2 2v4h4v-2h2v2h8v-2h2v2h4v-4c0-1.1-.9-2-2-2m-5 0H9V6h6z"/></svg> <span class=md-ellipsis> Tools </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Tools </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex=0> <span class=md-ellipsis> Cheat Sheet </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> Cheat Sheet </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/Tools%20%E5%BE%85%E6%95%B4%E7%90%86/ class=md-nav__link> <span class=md-ellipsis> Tools 待整理 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/ class=md-nav__link> <span class=md-ellipsis> 常用命令 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/ class=md-nav__link> <span class=md-ellipsis> 常用配置文件 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2_5> <label class=md-nav__link for=__nav_5_2_5 id=__nav_5_2_5_label tabindex=0> <span class=md-ellipsis> tools </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2_5> <span class="md-nav__icon md-icon"></span> tools </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/adb%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> adb Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/bash%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> bash Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/ffmpeg%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> ffmpeg Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/gdb%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> gdb Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/git%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> git </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/ip%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> ip Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/tmux%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> tmux Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/Docker%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> Docker Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/KDE%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> KDE </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/Makeflie%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> Makeflie </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/chezmoi%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> 完整的 Dotfiles 配置指南 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/mkdocs%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> mkdocs Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/network%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> network Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/ssh%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> SSH配置指南 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/tabby%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> Tabby 终端配置完全指南 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/wandb%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> wandb Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/zotero%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> zotero 使用指南 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/%E5%B0%8F%E9%B9%A4%E5%8F%8C%E6%8B%BC%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> 小鹤双拼 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> 远程连接 Cheat Sheet </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2_6> <label class=md-nav__link for=__nav_5_2_6 id=__nav_5_2_6_label tabindex=0> <span class=md-ellipsis> editors </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2_6> <span class="md-nav__icon md-icon"></span> editors </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/emacs%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> emacs Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/nano%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> nano Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/VScode%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> VScode </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/org%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> org Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/vim%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> vim Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/nvim%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> nvim </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/cursor%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> cursor </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2_7> <label class=md-nav__link for=__nav_5_2_7 id=__nav_5_2_7_label tabindex=0> <span class=md-ellipsis> languages </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2_7> <span class="md-nav__icon md-icon"></span> languages </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/javascript%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> javascript Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/python%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> Python Numpy </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/vimscript%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> VimScript Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/Go%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> Go Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/markdown%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> markdown Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/typst%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> typst Cheat Sheet </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2_8> <label class=md-nav__link for=__nav_5_2_8 id=__nav_5_2_8_label tabindex=0> <span class=md-ellipsis> System </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2_8> <span class="md-nav__icon md-icon"></span> System </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/System/Arch%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> arch 配置 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex=0> <span class=md-ellipsis> AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/AI/prompt/ class=md-nav__link> <span class=md-ellipsis> prompt </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/AI/prompt_writing/ class=md-nav__link> <span class=md-ellipsis> AI 使用 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex=0> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Blog/Mkdocs_Material/ class=md-nav__link> <span class=md-ellipsis> mkdocs material 超全配置 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex=0> <span class=md-ellipsis> Environment </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> Environment </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Environment/Arch_setup/ class=md-nav__link> <span class=md-ellipsis> arch 配置 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Environment/Ubuntu_setup/ class=md-nav__link> <span class=md-ellipsis> Ubuntu 配置 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Environment/environment/ class=md-nav__link> <span class=md-ellipsis> Environment </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Environment/obsidian_setup/ class=md-nav__link> <span class=md-ellipsis> obsidian 配置 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_6> <label class=md-nav__link for=__nav_5_6 id=__nav_5_6_label tabindex=0> <span class=md-ellipsis> Make </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6> <span class="md-nav__icon md-icon"></span> Make </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Make/CMake/ class=md-nav__link> <span class=md-ellipsis> CMake 相关 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Make/Makeflie/ class=md-nav__link> <span class=md-ellipsis> Makeflie </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_7> <label class=md-nav__link for=__nav_5_7 id=__nav_5_7_label tabindex=0> <span class=md-ellipsis> Others </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_7_label aria-expanded=false> <label class=md-nav__title for=__nav_5_7> <span class="md-nav__icon md-icon"></span> Others </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Others/Chezmoi/ class=md-nav__link> <span class=md-ellipsis> 用 chezmoi 实现跨设备同步配置 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Others/SSH/ class=md-nav__link> <span class=md-ellipsis> SSH配置指南 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Others/zotero_%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/ class=md-nav__link> <span class=md-ellipsis> zotero_使用指南 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_8> <label class=md-nav__link for=__nav_5_8 id=__nav_5_8_label tabindex=0> <span class=md-ellipsis> Terminal </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_8> <span class="md-nav__icon md-icon"></span> Terminal </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Terminal/Tabby_Zsh/ class=md-nav__link> <span class=md-ellipsis> Tabby + Zsh 配置指南 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../tags/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5.5 7A1.5 1.5 0 0 1 4 5.5 1.5 1.5 0 0 1 5.5 4 1.5 1.5 0 0 1 7 5.5 1.5 1.5 0 0 1 5.5 7m15.91 4.58-9-9C12.05 2.22 11.55 2 11 2H4c-1.11 0-2 .89-2 2v7c0 .55.22 1.05.59 1.41l8.99 9c.37.36.87.59 1.42.59s1.05-.23 1.41-.59l7-7c.37-.36.59-.86.59-1.41 0-.56-.23-1.06-.59-1.42"/></svg> <span class=md-ellipsis> Tags </span> </a> </li> <li class=md-nav__item> <a href=../../../links/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.78 3.653a3.936 3.936 0 1 1 5.567 5.567l-3.627 3.627a3.936 3.936 0 0 1-5.88-.353.75.75 0 0 0-1.18.928 5.436 5.436 0 0 0 8.12.486l3.628-3.628a5.436 5.436 0 1 0-7.688-7.688l-3 3a.75.75 0 0 0 1.06 1.061z"/><path d="M7.28 11.153a3.936 3.936 0 0 1 5.88.353.75.75 0 0 0 1.18-.928 5.436 5.436 0 0 0-8.12-.486L2.592 13.72a5.436 5.436 0 1 0 7.688 7.688l3-3a.75.75 0 1 0-1.06-1.06l-3 3a3.936 3.936 0 0 1-5.567-5.568z"/></svg> <span class=md-ellipsis> Links </span> </a> </li> <li class=md-nav__item> <a href=../../../about/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-91.9-186.5C182 346.2 212.6 368 256 368s74-21.8 91.9-42.5c5.8-6.7 15.9-7.4 22.6-1.6s7.4 15.9 1.6 22.6c-22.3 25.6-61 53.5-116.1 53.5s-93.8-27.9-116.1-53.5c-5.8-6.7-5.1-16.8 1.6-22.6s16.8-5.1 22.6 1.6M144.4 208a32 32 0 1 1 64 0 32 32 0 1 1-64 0m156.4 25.6c-5.3 7.1-15.3 8.5-22.4 3.2s-8.5-15.3-3.2-22.4c30.4-40.5 91.2-40.5 121.6 0 5.3 7.1 3.9 17.1-3.2 22.4s-17.1 3.9-22.4-3.2c-17.6-23.5-52.8-23.5-70.4 0"/></svg> <span class=md-ellipsis> About </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 【课程目录】 </span> </a> </li> <li class=md-nav__item> <a href=#p1 class=md-nav__link> <span class=md-ellipsis> P1 强化学习基础 </span> </a> <nav class=md-nav aria-label="P1 强化学习基础"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#terminologies class=md-nav__link> <span class=md-ellipsis> 一、名词（Terminologies） </span> </a> </li> <li class=md-nav__item> <a href=#return-and-value class=md-nav__link> <span class=md-ellipsis> 二、Return and Value </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p2-value-based-reinforcement-learning class=md-nav__link> <span class=md-ellipsis> P2 价值学习 (Value-Based Reinforcement Learning) </span> </a> <nav class=md-nav aria-label="P2 价值学习 (Value-Based Reinforcement Learning)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#deep-q-networkdqn class=md-nav__link> <span class=md-ellipsis> 一、Deep Q-Network(DQN) </span> </a> </li> <li class=md-nav__item> <a href=#temporal-difference-learning class=md-nav__link> <span class=md-ellipsis> 二、时间差分算法(Temporal Difference Learning) </span> </a> </li> <li class=md-nav__item> <a href=#td-dqntd-learning-for-dqn class=md-nav__link> <span class=md-ellipsis> 三、用 TD 算法训练 DQN(TD Learning for DQN) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p3-policy-based-reinforcement-learning class=md-nav__link> <span class=md-ellipsis> P3 策略学习 (Policy-Based Reinforcement Learning) </span> </a> <nav class=md-nav aria-label="P3 策略学习 (Policy-Based Reinforcement Learning)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 一、策略函数近似 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 二、策略学习的目标函数 </span> </a> </li> <li class=md-nav__item> <a href=#policy-gradient class=md-nav__link> <span class=md-ellipsis> 三、策略梯度(Policy Gradient) </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 四、使用策略梯度更新策略网络 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p4-actor-critic-methods class=md-nav__link> <span class=md-ellipsis> P4 运动员-裁判算法 (Actor-Critic Methods) </span> </a> <nav class=md-nav aria-label="P4 运动员-裁判算法 (Actor-Critic Methods)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 一、价值函数近似 </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 二、训练流程 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p5-alphago class=md-nav__link> <span class=md-ellipsis> P5 AlphaGo </span> </a> <nav class=md-nav aria-label="P5 AlphaGo"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#alphago class=md-nav__link> <span class=md-ellipsis> 一、AlphaGo设计思路 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> 二、蒙特卡洛树搜索主要思想 </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> 三、蒙特卡洛树搜索四步骤 </span> </a> </li> <li class=md-nav__item> <a href=#alphago-zero-alphago class=md-nav__link> <span class=md-ellipsis> 四、AlphaGo Zero 与 AlphaGo </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p6 class=md-nav__link> <span class=md-ellipsis> P6 蒙特卡罗方法 </span> </a> <nav class=md-nav aria-label="P6 蒙特卡罗方法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_9 class=md-nav__link> <span class=md-ellipsis> 一、 蒙特卡罗方法算法简介 </span> </a> </li> <li class=md-nav__item> <a href=#_10 class=md-nav__link> <span class=md-ellipsis> 二、时间差分学习与蒙特卡罗方法对比 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p7-fisher-yates class=md-nav__link> <span class=md-ellipsis> P7 随机排列与Fisher-Yates算法 </span> </a> <nav class=md-nav aria-label="P7 随机排列与Fisher-Yates算法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_11 class=md-nav__link> <span class=md-ellipsis> 一、随机排列问题 </span> </a> </li> <li class=md-nav__item> <a href=#fisher-yates class=md-nav__link> <span class=md-ellipsis> 二、Fisher-Yates算法 </span> </a> </li> <li class=md-nav__item> <a href=#fisher-yates_1 class=md-nav__link> <span class=md-ellipsis> 三、改进版Fisher-Yates算法过程 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p8-sarsa class=md-nav__link> <span class=md-ellipsis> P8 Sarsa算法 </span> </a> <nav class=md-nav aria-label="P8 Sarsa算法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sarsatabular-version class=md-nav__link> <span class=md-ellipsis> 一、Sarsa(Tabular Version) </span> </a> </li> <li class=md-nav__item> <a href=#sarsavalue-network-version class=md-nav__link> <span class=md-ellipsis> 二、Sarsa(Value Network Version) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p9-q-learning class=md-nav__link> <span class=md-ellipsis> P9 Q-Learning算法 </span> </a> <nav class=md-nav aria-label="P9 Q-Learning算法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#q-learningtabular-version class=md-nav__link> <span class=md-ellipsis> 一、Q-Learning(Tabular Version) </span> </a> </li> <li class=md-nav__item> <a href=#q-learningdqn-version class=md-nav__link> <span class=md-ellipsis> 二、Q-Learning(DQN Version) </span> </a> </li> <li class=md-nav__item> <a href=#sarsa-q-learning class=md-nav__link> <span class=md-ellipsis> 三、Sarsa 与 Q-Learning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p10-multi-step-td-target class=md-nav__link> <span class=md-ellipsis> P10 Multi-Step TD Target </span> </a> </li> <li class=md-nav__item> <a href=#p11-experience-replay class=md-nav__link> <span class=md-ellipsis> P11 经验回放 Experience Replay </span> </a> <nav class=md-nav aria-label="P11 经验回放 Experience Replay"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#experience-replay class=md-nav__link> <span class=md-ellipsis> 一、Experience Replay </span> </a> </li> <li class=md-nav__item> <a href=#prioritized-experience-replay class=md-nav__link> <span class=md-ellipsis> 二、Prioritized Experience Replay </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#p12-target-networkdouble-d class=md-nav__link> <span class=md-ellipsis> P12 高估问题、Target Network、Double D </span> </a> <nav class=md-nav aria-label="P12 高估问题、Target Network、Double D"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_12 class=md-nav__link> <span class=md-ellipsis> 一、高估问题 </span> </a> </li> <li class=md-nav__item> <a href=#target-network class=md-nav__link> <span class=md-ellipsis> 二、Target Network </span> </a> </li> <li class=md-nav__item> <a href=#double-d class=md-nav__link> <span class=md-ellipsis> 三、Double D </span> </a> </li> <li class=md-nav__item> <a href=#td-target class=md-nav__link> <span class=md-ellipsis> 四、三种TD target计算方式对比 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>王树森Reinforcement_Learning学习笔记</h1> <div><div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;"> <p><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约 7722 个字 <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 17H7V3h14m0-2H7a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2M3 5H1v16a2 2 0 0 0 2 2h16v-2H3m12.96-10.71-2.75 3.54-1.96-2.36L8.5 15h11z"></path></svg></span> 36 张图片 <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间 39 分钟 <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5"></path></svg></span> 共被读过 <span id=busuanzi_value_page_pv></span> 次</p> </div> <div class=md-typeset> <div class=blogging-tags-grid> <a href=https://tendourisu.github.io/tags#RL class=blogging-tag><code>#RL</code></a> </div> </div> <style>
    .md-typeset .blogging-tags-grid {
        display: flex;
        flex-direction: row;
        flex-wrap: wrap;
        gap: 8px;
        margin-top: 5px;
    }

    .md-typeset .blogging-tag {
        color: var(--md-typeset-color);
        background-color: var(--md-typeset-code-color);
        white-space: nowrap;
        display: block;
    }

    .md-typeset .blogging-tag code {
        border-radius: 5px;
    }
</style> <h2 id=_1>【课程目录】<a class=headerlink href=#_1 title="Permanent link">¶</a></h2> <p><strong>P1 强化学习基础（Reinforcement Learning）：</strong> 学习强化学习相关的基本概念</p> <p><strong>P2 价值学习（Value-based learning）：</strong> 学习DQN，以及DQN的训练方法——时序差分方法</p> <p><strong>P3 策略学习（Policy-based learning）：</strong> 学习策略网络、策略梯度以及如何更新策略网络相关知识</p> <p><strong>P4 运动员-裁判算法（Actor-critic method）：</strong> 学习最优动作价值函数近似方法<strong>——</strong>Actor-Critic 算法</p> <p><strong>P5 AlphaGo：</strong> 学习AlphaGo的训练与决策方法</p> <p><strong>P6 蒙特卡罗方法：</strong> 学习蒙特卡罗方法</p> <p><strong>P7 随机排列与Fisher-Yates算法：</strong></p> <p><strong>P8 Sarsa算法：</strong> 学习TD算法中的Sarsa算法</p> <p><strong>P9 Q-Learning算法：</strong> 学习TD算法中的Q-Learning算法</p> <p><strong>P10 Multi-Step TD Target：</strong></p> <p><strong>P11 经验回放 Experience Replay ：</strong></p> <p><strong>P12 高估问题、Target Network、Double D：</strong></p> <hr> <h2 id=p1>P1 强化学习基础<a class=headerlink href=#p1 title="Permanent link">¶</a></h2> <h3 id=terminologies>一、名词（Terminologies）<a class=headerlink href=#terminologies title="Permanent link">¶</a></h3> <p><strong>1.智能体(Agent)：</strong> 强化学习的主体</p> <p><strong>2.环境(Environment)：</strong> 与智能体进行交互的对象，可以抽象地理解为交互过程中的规则或机制。</p> <p><strong>3.状态(State)/观测(Observation)：</strong> 在每个时刻，环境有一个状态 (state)，状态(State) 有时也被称为观测(Observation)。（因为有时智能体并不能观测到环境改变后的全部，只能观测到部分）</p> <p><strong>4.状态空间(State Space)：</strong> 所有可能存在状态的集合，记作花体字母 <span class=arithmatex>\(\mathcal{S}\)</span> 。</p> <blockquote> <p>注意：状态空间可以是离散的，也可以是连续的。状态空间可以是有限集合，也可以是无限可数集合。</p> </blockquote> <p><strong>5.动作(Action)：</strong> 动作 (action) 是智能体基于当前状态所做出的决策。</p> <p><strong>6.动作空间(Action Apace)：</strong> 指所有可能动作的集合，记作花体字母 <span class=arithmatex>\(\mathcal{A}\)</span> 。</p> <p><strong>7.奖励 (reward)：</strong> 是指在智能体执行一个动作之后，环境返回给智能体的一个数值。</p> <p><strong>8.策略 (policy) ：</strong> 根据观测到的状态，如何做出决策，即如何从动作空间中选取一个动作。</p> <blockquote> <p>注意：<br> （1）强化学习的目标就是得到一个<strong>策略函数 (policy function)</strong>，也叫<span class=arithmatex>\(\pi\)</span> 函数 ( <span class=arithmatex>\(\pi\)</span> function) ，在每个时刻根据观测到的状态做出决策。<br> （2）<span class=arithmatex>\(\pi\)</span>函数输入的是状态，输出的是动作的概率，例如下面up的概率为0.7。</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-51699d11055e5f0d34b6a6ee33890ba6_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-51699d11055e5f0d34b6a6ee33890ba6_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-51699d11055e5f0d34b6a6ee33890ba6_1440w.jpg></a></p> <p><strong>10.状态转移概率函数：</strong> 描述状态转移的函数，有如下2两种表达式： </p> <div class=arithmatex>\[ P_{ss'}^{a} \color{default}=P[S_{t+1} = s'|S_{t}=s,A_{t} =a] \]</div> <div class=arithmatex>\[ p_t(s' \mid s, a) = P(S_{t+1} = s' \mid S_t = s, A_t = a) \]</div> <blockquote> <p>解释：当前状态 <span class=arithmatex>\(s\)</span> ，当前智能体执行动作 <span class=arithmatex>\(a\)</span>，在 $t+1 $ 时刻环境状态变成 <span class=arithmatex>\(s'\)</span> 的概率是多少。（P表示概率）</p> </blockquote> <p><strong>11.马尔可夫决策过程 (Markov decision process, MDP)：</strong> 强化学习的数学基础和建模工具。一个 MDP 通常由状态空间、动作空间、状态转移函数、奖励函数、折扣因子等组成，其可用元组 <span class=arithmatex>\(&lt;S,A,P,R,γ&gt;\)</span> 进行表示。各部分含义如下：</p> <ul> <li><span class=arithmatex>\(S\)</span>：有限数量的状态集</li> <li><strong><span class=arithmatex>\(A\)</span> ：有限数量的动作集</strong></li> <li>$P $ ：是态转移概率矩阵 <span class=arithmatex>\(P_{ss'}^{a} \color{default}=P[S_{t+1} = s'|S_{t}=s,A_{t} =a]\)</span></li> <li><span class=arithmatex>\(R\)</span> ：奖励函数 $ R_{s}^a = E[R_{t+1} | S_{t} = s,A_{t} =a] $</li> <li><span class=arithmatex>\(\gamma\)</span>：折扣因子，<span class=arithmatex>\(\gamma\in\)</span><span class=arithmatex>\(\gamma\in\)</span> [0, 1]</li> </ul> <blockquote> <p>注意：<br> （1）马尔可夫<strong>奖励过程</strong>的状态转移概率和奖励函数<strong>仅取决于当前状态。</strong><br> （2）马尔可夫<strong>决策过程</strong>的状态转移概率和奖励函数<strong>不仅取决于智能体当前状体，还取决于智能体选取的动作</strong>。</p> </blockquote> <p>例子：学生马尔可夫决策过程</p> <blockquote> <p>解释：黄色字体表示学生采取的动作，框图表示MRP的状态名（避免混淆隐去），R表示奖励函数，其与学生所采取的动作有关。<br> 注意：当学生选择“去查阅文献pub”这个动作时，则将进入一个临时状态（图中用黑色小实点表示），随后被环境按照其动力学分配到另外三个MRP状态（class1、class2、class3），也就是说此时Agent<strong>没有选择权</strong>决定去哪一个状态。</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-d5f797be692be95e48476449b2fde93f_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-d5f797be692be95e48476449b2fde93f_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-d5f797be692be95e48476449b2fde93f_1440w.jpg></a></p> <p>参考资料</p> <p><a href=https://zhuanlan.zhihu.com/p/494755866>李建平：第二讲 马尔可夫决策过程</a></p> <blockquote> <p>以下为课外补充</p> </blockquote> <p><strong>12.强化学习（reinforcement learning）：</strong> 其是一种实现序贯决策的机器学习方法，定义如下：<strong>机器通过与环境进行交互，不断尝试，从错误中学习，做出正确决策从而实现目标的方法。</strong></p> <blockquote> <p>注意：机器学习分为有监督学习方法、无监督学习方法、强化学习，三者并列。</p> </blockquote> <p><strong>13.强化学习分类：</strong></p> <p><strong>（1）根据agent学习方式分：</strong> 基于价值的强化学习Value based RL（P2）、基于策略的强化学习Policy based RL（P3） 、以及Actor-Critic方法（P4）。</p> <p><strong>（2）按agent有无学习环境的模型分：</strong> model-based（通过学习状态转移概率 <span class=arithmatex>\(P ( s , s ′ )\)</span> 采取行动）与model-free（通过学习价值函数 <span class=arithmatex>\(V π ( s )\)</span> 与策略函数进行决策）</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-ba993ffb12b83fd73b2a5f8ee757afae_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-ba993ffb12b83fd73b2a5f8ee757afae_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-ba993ffb12b83fd73b2a5f8ee757afae_1440w.jpg></a></p> <p><strong>（3）根据如何使用已有的数据进行学习和决策分：</strong> 在线策略（on-policy）和离线策略（off-policy）。</p> <ul> <li>在线策略（on-policy）算法表示<strong>行为策略和目标策略是同一个策略</strong>，agent根据当前的策略来选择动作，并且学习的目标是优化当前正在执行的策略</li> <li>离线策略（off-policy）算法表示行为<strong>策略和目标策略不是同一个策略</strong>，学习算法可以利用从其他策略生成的数据来进行学习，而不局限于当前执行的策略</li> </ul> <blockquote> <p>相比于在线策略，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度</p> </blockquote> <p><strong>（4）根据更新方式分：</strong> 回合更新（Monte-Carlo update）与单步更新（Temporal-DIfference update）。</p> <ul> <li>回合更新：游戏有开始和结束，回合更新只有等待一局游戏从开始到结束，然后才能更新行为准则</li> <li>单步更新：在游戏过程中，每一步都可以更新，不用等待游戏的结束，边玩边学习，学习效率更好</li> </ul> <p><strong>14.Rollout</strong> 在强化学习中，rollout指的是在训练过程中，智能体根据当前的策略在环境中进行一系列的<strong>模拟交互步骤，模拟并收集样本数据的过程。</strong></p> <blockquote> <p>理解：仿真器（webots、mujoco）是提供虚拟环境和交互的<strong>工具</strong>，而 rollout 是在该环境中进行交互并收集样本数据的<strong>过程</strong>。</p> </blockquote> <p><strong>15.回合（episode）：</strong> 一个完整的任务执行过程被称为一个回合或一个episode，强调任务从开始到终止的全部过程。</p> <p><strong>16.转移（transition）：</strong> 在强化学习中，一个状态到另一个状态的变化被称为转移。转移通常由三元组（也可以包括奖励信号）表示(state, action, next_state)</p> <p><strong>17.轨迹（trajectory）：</strong> 轨迹是一连串的<strong>状态-动作对</strong>，描述了智能体是如何在环境中移动并与环境互动的。<strong>在一个回合中，可以包含多个轨迹，而一个轨迹可以包含一系列转移组成。</strong></p> <p>参考资料：</p> <p><a href="https://blog.csdn.net/Ever_____/article/details/133362585?ops_request_misc=%7B%22request%5Fid%22%3A%22abfc536d828c6cb4ad3004cf9dcc7559%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=abfc536d828c6cb4ad3004cf9dcc7559&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-26-133362585-null-null.142^v100^pc_search_result_base1&utm_term=Rollout&spm=1018.2226.3001.4187">【强化学习】强化学习基础教程：基本概念、强化学习的定义，要素，方法分类 以及 Rollout、episode回合、transition转移、trajectory轨迹的概念</a></p> <h3 id=return-and-value>二、Return and Value<a class=headerlink href=#return-and-value title="Permanent link">¶</a></h3> <p><strong>1.回报(Return，也叫累计奖励)</strong> 顾名思义，从当前时刻开始到本回合结束的所有奖励的总和。计 <span class=arithmatex>\(t\)</span> 时刻的回报为<strong>随机变量</strong> <span class=arithmatex>\(U_{t}\)</span> ： <span class=arithmatex>\(U_{t} = R_{t} + R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_{n}\)</span></p> <blockquote> <p>注意：强化学习的目标就是寻找一个策略，使得回报的期望最大化，这个策略称为最优策略 (optimum policy)。</p> </blockquote> <p><strong>2.折扣回报(Discounted Return)</strong> 在 MDP 中，通常使用折扣回报，即给未来的奖励做折扣。折扣回报的定义如下:</p> <div class=arithmatex>\[ U_t = R_t + \gamma R_{t+1} + \gamma ^2R_{t+2} + \gamma ^3R_{t+3} + ... \]</div> <blockquote> <p>注意：对待越久远的未来，折扣因子的幂越大，给奖励打的折扣越大。</p> </blockquote> <p><strong>3.动作价值函数(Action-value function)：</strong> 动作价值函数通常表示为 $ Q_π(s_t,a_t)$ ，它用于评估在给定状态 <span class=arithmatex>\(s_t\)</span> 下采取某个动作 $a_t $ 后，按照策略 <span class=arithmatex>\(π\)</span> 所能获得的期望回报。</p> <blockquote> <p>含义解释：动作价值函数像是评估你目前走的这步棋的得分，它考虑了你走这步棋后，按照你的策略继续下棋所能获得的期望得分。通过评估和比较当前不同动作的Q值，来决定走哪步棋。</p> </blockquote> <div class=arithmatex>\[ Q_{\pi}(S_t, A_t) = E_{S_{t+1}, A_{t+1}, \ldots, S_n, A_n} \left[ U_t \mid S_t = s_t, A_t = a_t \right] \]</div> <blockquote> <p>注意：<br> （1）动作价值函数只依赖于当前状态 <span class=arithmatex>\(s_t,\)</span>动作 $a_t $ ​，而不依赖于之后的状态和动作。<br> （2）在强化学习中，动作价值函数用于评估不同动作的优劣，并指导智能体做出决策。智能体通常会选择那些能够最大化Q的动作。<br> （3）不论未来采取什么样的策略<span class=arithmatex>\(\pi\)</span> ，回报<span class=arithmatex>\(U_t\)</span> 的期望不可能超过某个动作价值函数 <span class=arithmatex>\(Q^\star\)</span>，这个动作价值函数则称为<strong>最优动作价值函数(Optimal action-value function)</strong>。</p> </blockquote> <div class=arithmatex>\[ Q^\star(s_t,a_t) = max_\pi{Q_\pi(s_t,a_t)} \]</div> <p><strong>4.状态价值函数(State-value function)：</strong> 状态价值函数通常表示为 <span class=arithmatex>\(V_π​(s_t​)\)</span> ，它用于评估在给定状态状态 <span class=arithmatex>\(s_t\)</span> 下，按照策略 <span class=arithmatex>\(π\)</span> 所能获得的期望回报。</p> <blockquote> <p>含义解释：与动作价值函数不同状态价值函数不关心在特定状态下采取的具体动作，而是关心在该状态下遵循策略所能获得的平均回报。</p> </blockquote> <div class=arithmatex>\[ V_{\pi}(s_{t}) = \mathbb{E}_{A_{t} \sim \pi(.|s_{t})} \left[ Q_{\pi}(s_{t}, A_{t}) \right] = \sum_{a \in \mathcal{A}} \pi(a \mid s_{t}) Q_{\pi}(s_{t}, a) \]</div> <blockquote> <p>注意：<br> （1）状态价值函数依赖于策略，这意味着它反映了在特定策略下，从某个状态开始所能获得的期望回报。<br> （2）状态价值函数可以用来衡量策略 与状态的好坏。状态价值越大，意味着在该状态下遵循策略所能获得的期望回报越高，策略越好。<br> （3）上述是离散的写法（连加），如果是连续的需要换成定积分。<br> （3） <strong><span class=arithmatex>\({\pi}\)</span> 与 <span class=arithmatex>\(Q_{\pi}\)</span> 实际上我们并不知道，</strong>只能用近似的方式获得。例如P4介绍的Actor-Critic方法中，就使用了两个神经网络去近似，然后用Actor-Critic方法同时学习者两个神经网络。</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-f54d9ed2330d82ca8314fffa057379a2_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-f54d9ed2330d82ca8314fffa057379a2_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-f54d9ed2330d82ca8314fffa057379a2_1440w.jpg></a></p> <p><a href=https://zhuanlan.zhihu.com/p/588047970>codingpath：王树森老师 DRL 课程笔记 P1-强化学习 (Reinforcement Learning) 基本概念</a></p> <hr> <h2 id=p2-value-based-reinforcement-learning>P2 价值学习 (Value-Based Reinforcement Learning)<a class=headerlink href=#p2-value-based-reinforcement-learning title="Permanent link">¶</a></h2> <h3 id=deep-q-networkdqn>一、Deep Q-Network(DQN)<a class=headerlink href=#deep-q-networkdqn title="Permanent link">¶</a></h3> <p>在前面的学习我们知道，不论未来采取什么样的策略<span class=arithmatex>\(\pi\)</span> ，回报<span class=arithmatex>\(U_t\)</span> 的期望不可能超过最优动作价值函数<span class=arithmatex>\(Q^\star\)</span>。那么我就可以根据根据 <span class=arithmatex>\(Q^\star\)</span>的值选择最优动作，然后最大化未来的累计奖励。但事实上我们并不知道<span class=arithmatex>\(Q^\star\)</span> 的函数表达式，只能近似学习 <span class=arithmatex>\(Q^\star\)</span>。一种常见的办法就是使用<strong>Deep Q Network(DQN，神经网络)。</strong></p> <div class=arithmatex>\[ s_t\rightarrow{}Q(s,a;w)\rightarrow{}Q^* \]</div> <blockquote> <p>注意：<span class=arithmatex>\(w\)</span>为神经网络中的参数。在训练DQN时，我首先需要随机初始化它，然后使用时间差分算法等算法逐步去更新它。</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-9059fc19d74d4dc361bd8f8290291183_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-9059fc19d74d4dc361bd8f8290291183_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-9059fc19d74d4dc361bd8f8290291183_1440w.jpg></a></p> <blockquote> <p>注意：<br> （1）DQN输入：输入是状态 s。这个例子中是一个游戏的截图。<br> （2）DQN输出：离散动作空间<span class=arithmatex>\(\mathcal{A}\)</span> 上的每个动作的 Q 值，即给每个动作的评分，分数越高意味着动作越好。例如下图中UP的Q值最高。</p> </blockquote> <p>我们可以将 DQN 应用到玩游戏当中，在游戏的每一个回合中，我们总是根据DQN选出Q值最大的动作<span class=arithmatex>\(a^\star = argmax_{a}Q^\star(s, a)\)</span>，转移到下一个状态，并重复这一过程，直到游戏结束。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-1e49fd1e4e266d76236278416412aff8_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-1e49fd1e4e266d76236278416412aff8_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-1e49fd1e4e266d76236278416412aff8_1440w.jpg></a></p> <blockquote> <p>注意：<br> 这种方法被称为贪婪策略（greedy policy），即在每个时间步都选择当前最优的动作。在实际应用中，为了探索更多的动作并避免过早地陷入局部最优，可能会使用ε-贪婪策略（ε-greedy policy），即以一定的概率ε随机选择一个动作以探索环境，而不是总是选择当前最优的动作。</p> </blockquote> <h3 id=temporal-difference-learning>二、时间差分算法(Temporal Difference Learning)<a class=headerlink href=#temporal-difference-learning title="Permanent link">¶</a></h3> <p>训练 <strong>DQN</strong> 最常用的算法是时间差分 (temporal difference，缩写 TD) 算法。时间差分学习是一种在线学习方法，<strong>即在每一步中都会更新价值函数</strong>。其关键思想是用下一个状态的价值来<strong>估计</strong>当前状态的价值，并据此进行更新。</p> <p>TD算法的损失函数：<span class=arithmatex>\(L(w) = \frac{1}{2}\left [Q(s,d;w) - y \right ]^2 \\\)</span></p> <blockquote> <p>其中： <span class=arithmatex>\([Q(s,d;w) - y]\)</span>称为TD 误差(TD error)<strong>。</strong></p> </blockquote> <p>然后使用链式法则对 <span class=arithmatex>\(L(w)\)</span>关于参数 <span class=arithmatex>\(w\)</span>求梯度：</p> <div class=arithmatex>\[ \frac{\partial{L}}{\partial{w}} = \frac{\partial{q}}{\partial{w}} \cdot \frac{\partial{L}}{\partial{q}} = (q-y) \cdot \frac{\partial{Q(w)}}{\partial{w}} \]</div> <p>最后做梯度下降更新模型参数<span class=arithmatex>\(w\)</span> :</p> <div class=arithmatex>\[ w_{t+1} = w_t - \alpha \cdot \frac{\partial{L}}{\partial w} \mid _{w=w_t} \]</div> <blockquote> <p>此处的 <span class=arithmatex>\(\alpha\)</span>是学习率，是个超参数，需要手动调。</p> </blockquote> <h3 id=td-dqntd-learning-for-dqn>三、用 TD 算法训练 DQN(TD Learning for DQN)<a class=headerlink href=#td-dqntd-learning-for-dqn title="Permanent link">¶</a></h3> <p>（1）首先用价值网络 <span class=arithmatex>\(Q\)</span> 给动作 <span class=arithmatex>\(a_t\)</span> 与 <span class=arithmatex>\(a_{t+1}\)</span> 打分</p> <blockquote> <p>注意：这里的动作是根据策略网络随机抽样得到的</p> </blockquote> <p>（2）计算TDtarget，把计算出来的结果记作 <span class=arithmatex>\(y\)</span> </p> <blockquote> <p>注意<br> （1）<span class=arithmatex>\(\gamma\)</span>是折扣率，让未来的奖励没有当前奖励的权重高<br> （2）<span class=arithmatex>\(y_t\)</span>和 <span class=arithmatex>\(Q(s_t,a_t;w_t)\)</span> 实际上都是对未来奖励综合的估计，但是<span class=arithmatex>\(y_t\)</span>由一部分真实观测到的值 <span class=arithmatex>\(r_t\)</span> 组成，因此他的精度更高。<strong>TD算法的核心就是用<span class=arithmatex>\(Q(s_t,a_t;w_t)\)</span>做预测，将<span class=arithmatex>\(y_t\)</span>做为目标，让预测尽可能的接近目标。</strong></p> </blockquote> <p>（3）计算损失函数Loss（即两种估计作差并求平方，鼓励<span class=arithmatex>\(Q(s_t,a_t;w_t)\)</span>尽量接近<span class=arithmatex>\(y_t\)</span>）</p> <p>（4）做梯度下降更新参数</p> <blockquote> <p>注意：用梯度下降更新 <span class=arithmatex>\(w\)</span> 后，损失函数会逐步变小，训练结果越接近目标。</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-9a3aff4e6d7ff80dcf81ab966410ff0c_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-9a3aff4e6d7ff80dcf81ab966410ff0c_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-9a3aff4e6d7ff80dcf81ab966410ff0c_1440w.jpg></a></p> <p>参考深度强化学习(4_5)：Actor-Critic Methods 10：00</p> <p><strong>参考资料：</strong></p> <p><a href=https://zhuanlan.zhihu.com/p/658564004>Native8418：什么是时间差分（Temporal Difference）学习，它如何与蒙特卡罗方法相比？</a></p> <hr> <h2 id=p3-policy-based-reinforcement-learning>P3 策略学习 (Policy-Based Reinforcement Learning)<a class=headerlink href=#p3-policy-based-reinforcement-learning title="Permanent link">¶</a></h2> <h3 id=_2>一、策略函数近似<a class=headerlink href=#_2 title="Permanent link">¶</a></h3> <p>在强化学习基础的学习我们知道，如果我们有一个非常好的策略函数 <span class=arithmatex>\(\pi(a \mid s)\)</span> ，我们就可以根据该策略函数计算出每个动作的概率值，从而控制智能体。</p> <p>然而策略函数与最优动作价值函数 <span class=arithmatex>\(Q^\star\)</span> 一样，我们并不知道他们的具体的函数值，因此我们可以使用<strong>神经网络</strong>近似策略函数。我们把这个神经网络称为<strong>策略网络(policy network)</strong> 记作 <span class=arithmatex>\(\pi(a \mid s;\theta)\)</span> ；将近似得到的函数称为<strong>策略函数</strong> ，记作 <span class=arithmatex>\(\pi(a \mid s)\)</span> <strong>。</strong></p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-74d26413756af98ffbb7891a61725dda_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-74d26413756af98ffbb7891a61725dda_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-74d26413756af98ffbb7891a61725dda_1440w.jpg></a></p> <blockquote> <p>注意：<span class=arithmatex>\(\theta\)</span>表示神经网络的参数。一开始我们随机初始化 <span class=arithmatex>\(\theta\)</span>，随后我们利用收集的状态、动作、奖励不断更新 <span class=arithmatex>\(\theta\)</span>，直到策略函数近似程度满足我们的需求。</p> </blockquote> <p>策略网络的结果如下图所示，策略网络的输入是状态 <span class=arithmatex>\(s\)</span>，经过卷积神经网络（Conv）处理得到画面的特征向量，然后经过全连接神经网络映射到维度为 <span class=arithmatex>\(n\)</span> 的向量 <span class=arithmatex>\(f\)</span> ，然后用<span class=arithmatex>\(softmax\)</span>激活函数，输出概率分布。</p> <blockquote> <p>注意：<br> （1）动作空间 <span class=arithmatex>\(\mathcal{A}\)</span> 的大小是多少，向量 <span class=arithmatex>\(f\)</span>的维度 <span class=arithmatex>\(n\)</span> 就是多少。<br> （2）所以输出的向量所有元素都是正数（实际上是概率），且相加等于 1，即 <span class=arithmatex>\(\sum_{a \in \mathcal{A}}\pi(a \mid s; \theta) = 1\)</span></p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-04af3fc257b55e855fa3bf444a49e604_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-04af3fc257b55e855fa3bf444a49e604_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-04af3fc257b55e855fa3bf444a49e604_1440w.jpg></a></p> <h3 id=_3>二、策略学习的目标函数<a class=headerlink href=#_3 title="Permanent link">¶</a></h3> <p>如果用策略网络 <span class=arithmatex>\(\pi(a \mid s_t;\theta)\)</span> 去近似策略函数<span class=arithmatex>\(\pi(a \mid s_t)\)</span> ，此时对应的近似状态价值函数如下：</p> <div class=arithmatex>\[ V(s_t;\theta) = \sum_a\pi(a \mid s_t; \theta)\cdot Q_\pi(s_t,a) \]</div> <p>若一个策略足够好好，那么状态价值函数的近似<span class=arithmatex>\(V(s;\theta)\)</span> 的均值也越大。因此我们定义<strong>策略学习的目标函数</strong>： <span class=arithmatex>\(J(\theta) = E_S\left [V(S;\theta) \right]\)</span></p> <blockquote> <p>注意：目标函数<span class=arithmatex>\(J(\theta)\)</span> 排除了状态<span class=arithmatex>\(S\)</span> 的因素，只依赖于策略网络<span class=arithmatex>\(\pi\)</span> 的参数<span class=arithmatex>\(\theta\)</span> 。通过学习参数<span class=arithmatex>\(\theta\)</span> ，使得目标函数 <span class=arithmatex>\(J(\theta)\)</span> 越来越大，也就意味着策略网络越来越好。</p> </blockquote> <p>这里使用<strong>策略梯度上升</strong>来更新 <span class=arithmatex>\(\theta\)</span> ，使得 <span class=arithmatex>\(J(\theta)\)</span> 增大。设当前策略网络的参数为 <span class=arithmatex>\(\theta_{now}\)</span> ,做梯度上升更新参数，得到新的参数</p> <div class=arithmatex>\[ \theta_{new} : \theta_{new} \leftarrow \theta_{now} + \beta\cdot\nabla_\theta{J(\theta_{now})} \]</div> <p>此处的 <span class=arithmatex>\(\beta\)</span> 是学习率，需要手动调。其中的梯度：</p> <div class=arithmatex>\[ \nabla_\theta{J(\theta_{now})} = \frac{\partial J(\theta)}{\partial \theta} \mid _{\theta=\theta_{now}} \]</div> <p>称为<strong>策略梯度(policy gradient)</strong>。</p> <h3 id=policy-gradient>三、策略梯度(Policy Gradient)<a class=headerlink href=#policy-gradient title="Permanent link">¶</a></h3> <p>策略梯度计算公式如下所示，两者形式上等价。前者适用于动作离散的情形，后者适用于连续的动作。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-26c84e3f4f3cdd5d6804b035d5b2ecb7_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-26c84e3f4f3cdd5d6804b035d5b2ecb7_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-26c84e3f4f3cdd5d6804b035d5b2ecb7_1440w.jpg></a></p> <p>策略梯度两种计算方式</p> <p>例如当动作空间为 <span class=arithmatex>\(\mathcal{A} = \{left, right, up\}\)</span> ，我们使用form1进行计算，只需要计算处所有动作的 <span class=arithmatex>\(f(a,\theta)\)</span> ，将所有结果相加即可得到策略梯度。<br> <a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-202ebedd5999b422a2d002a7cee4242b_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-202ebedd5999b422a2d002a7cee4242b_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-202ebedd5999b422a2d002a7cee4242b_1440w.jpg></a></p> <p>对于连续的动作空间，如 <span class=arithmatex>\(\mathcal{A} = [0，1]\)</span> ，我们则使用forms2计算策略梯度。然而采用上述公式中计算策略梯度十分困难（期望、概率密度函数过于复杂），连加或定积分的计算量非常大。 因此我们可以通过蒙特卡洛法去近似该期望，计算的过程如下：<br> <a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-7d506449c067ab03e432874674c85e0b_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-7d506449c067ab03e432874674c85e0b_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-7d506449c067ab03e432874674c85e0b_1440w.jpg></a></p> <blockquote> <p>注意： <span class=arithmatex>\(\hat{a}\)</span> 是根据概率密度函数抽取出来的，所以<span class=arithmatex>\(g(\hat{a};\theta)\)</span> 是策略梯度的无偏估计，进一步的我们可以使用<span class=arithmatex>\(g(\hat{a};\theta)\)</span>近似策略梯度。</p> </blockquote> <h3 id=_4>四、使用策略梯度更新策略网络<a class=headerlink href=#_4 title="Permanent link">¶</a></h3> <p>算法步骤如下</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-1fc51b6f57be9f4268ffd402aee98a13_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-1fc51b6f57be9f4268ffd402aee98a13_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-1fc51b6f57be9f4268ffd402aee98a13_1440w.jpg></a> </p> <blockquote> <p>注意：在第 3 步中，我们是无法精确计算出 <span class=arithmatex>\(q_t\)</span> ，后续会介绍使用REINFORCE 算法、actor-critic 算法来近似<span class=arithmatex>\(Q_\pi(s,a)\)</span>。</p> </blockquote> <p>参考资料：</p> <p><a href=https://zhuanlan.zhihu.com/p/590289345>codingpath：王树森老师 DRL 课程笔记 P3-策略学习 (Policy-Based Reinforcement Learning)</a></p> <hr> <h2 id=p4-actor-critic-methods>P4 运动员-裁判算法 (Actor-Critic Methods)<a class=headerlink href=#p4-actor-critic-methods title="Permanent link">¶</a></h2> <h3 id=_5>一、价值函数近似<a class=headerlink href=#_5 title="Permanent link">¶</a></h3> <p>在前面我们提到，状态价值函数 <span class=arithmatex>\(V_\pi(s) = \sum_{a}\pi(a|s)\cdot Q_\pi(s,a)\)</span>中，<span class=arithmatex>\(\pi(a|s;\theta)\)</span>函数与<span class=arithmatex>\(q(s,a;w)\)</span>函数我们并不知道，<strong>因此我们需要用两个网络近似它们，并使用Actor-Critic算法同时学习这两个网络</strong>。</p> <p>Actor-Critic算法是价值学习和策略学习相结合起来，我们用策略网络近似<span class=arithmatex>\(\pi(a|s;\theta)\)</span>函数；用价值网络近似<span class=arithmatex>\(q(s,a;w)\)</span>函数。其中，Actor是策略网络，用来控制agent运动；Critic是价值网络，用来给价值打分。近似结果如黄色部分所示。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-8053823ff849e971f8fd59248636d594_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-8053823ff849e971f8fd59248636d594_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-8053823ff849e971f8fd59248636d594_1440w.jpg></a></p> <p>学习策略网络<span class=arithmatex>\(\pi(a|s;\theta)\)</span>是为了让运动员得到<strong>更高的平均分</strong>，即获得更高的状态价值函数 <span class=arithmatex>\(V_\pi(s)\)</span>；为了学习策略网络，我们需要价值网络<span class=arithmatex>\(q(s,a;w)\)</span>来当裁判，给运动员打分，学习价值网络主要是为了让裁判<strong>打分更加精准</strong>。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-409ecc9fe358bbfe6cc5d18ca7a886fa_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-409ecc9fe358bbfe6cc5d18ca7a886fa_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-409ecc9fe358bbfe6cc5d18ca7a886fa_1440w.jpg></a></p> <h3 id=_6>二、训练流程<a class=headerlink href=#_6 title="Permanent link">¶</a></h3> <p>（1）观测到旧的状态 <span class=arithmatex>\(s_t\)</span>，用策略网络计算概率分布，然后再根据概率随机抽取动作<span class=arithmatex>\(a_t\)</span>： <span class=arithmatex>\(a_t \sim \pi(\cdot | s_t;\theta_{now})\)</span></p> <p>（2）Agent执行动作<span class=arithmatex>\(a_t\)</span>，然后从环境中观测到奖励 <span class=arithmatex>\(r_t\)</span> 和新的状态 <span class=arithmatex>\(s_{t+1}\)</span>。</p> <p>（3）拿新的状态<span class=arithmatex>\(s_{t+1}\)</span>作为输入（回到第一步），根据策略网络计算出新的概率，并随机抽样得到动作<span class=arithmatex>\(a_{t+1}\)</span>： <span class=arithmatex>\(\tilde{a}_{t+1} \sim \pi(\cdot | s_{t+1};\theta_{now})\)</span>，<strong>但不让智能体执行动作 <span class=arithmatex>\(a_{t+1}\)</span>。</strong></p> <p>（4）计算用上述两次状态和动作让裁判打分输出：</p> <div class=arithmatex>\[ \hat{q_t} = q(s_t,a_t;w_{now}) 和 q_{t+1} = q(s_{t+1},\tilde{a}_{t+1};w_{now}) \]</div> <p>（5） 计算 TD 目标和 TD 误差：</p> <div class=arithmatex>\[ \hat{y_t} = r_t + \gamma \cdot \hat{q_{t+1}} 和 \delta_t = \hat{q_t} - \hat{y_t} \]</div> <blockquote> <p>注意： <span class=arithmatex>\(\hat{q_t}\)</span> 为当前预测， <span class=arithmatex>\(\hat{y_t}\)</span> 为TD目标，他们的差即为TD误差。</p> </blockquote> <p>（6）对价值网络求导，计算 <span class=arithmatex>\(Q\)</span> 网络关于参数 <span class=arithmatex>\(w\)</span> 梯度，记作 <span class=arithmatex>\(d_{w,t}\)</span> ：</p> <blockquote> <p>注意：<span class=arithmatex>\(d_{w,t}\)</span>与 <span class=arithmatex>\(w\)</span> 是同样大小的矩阵。</p> </blockquote> <p>（7）用TD算法梯度下降更新价值网络，让裁判打分更加精准：</p> <div class=arithmatex>\[ w_{new} = w_{now} - \alpha \cdot \delta_t \cdot d_{w,t} \]</div> <blockquote> <p>注意： <span class=arithmatex>\(\delta_t \cdot d_{w,t}\)</span> 表示梯度</p> </blockquote> <p>（8）对策略网络 <span class=arithmatex>\(\pi\)</span> 求导，记作<span class=arithmatex>\(d_{\theta,t}\)</span></p> <p>（9） 用梯度上升来更新策略网络，让运动员的平均分更高</p> <div class=arithmatex>\[ \theta_{new} = \theta_{now} + \beta \cdot q_t \cdot d_{\theta,t} \]</div> <blockquote> <p>注意：<br> （1） <span class=arithmatex>\(q_t \cdot d_{\theta,t}\)</span> 是策略梯度的蒙特卡罗近似， <span class=arithmatex>\(\beta\)</span> 为学习率。<br> （2）部分论文用TD误差 <span class=arithmatex>\(\delta_t\)</span> 进行更新，效果更好。</p> </blockquote> <div class=arithmatex>\[ \theta_{new} = \theta_{now} + \beta \cdot \delta_t \cdot d_{\theta,t} \]</div> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-a1f661508c2307075d8bce604486bf73_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-a1f661508c2307075d8bce604486bf73_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-a1f661508c2307075d8bce604486bf73_1440w.jpg></a></p> <hr> <h2 id=p5-alphago>P5 AlphaGo<a class=headerlink href=#p5-alphago title="Permanent link">¶</a></h2> <h3 id=alphago>一、AlphaGo设计思路<a class=headerlink href=#alphago title="Permanent link">¶</a></h3> <p>AlphaGo的设计思路如图所示，先用分<strong>三个步骤训练</strong>，再进行决策。</p> <p>（1）用16万局人类对决中采用<strong>behavior cloning</strong>来初步学习一个策略网络，让策略网络来<strong>模仿人类</strong>的动作</p> <p>（2）用强化学习（<strong>策略梯度算法</strong>）进一步训练该网络，让AlphaGo做自我博弈，用<strong>游戏胜负</strong>还更新策略网络（赢了奖励为1，输了奖励为-1）</p> <p>（3）第三步是训练价值网络，用来评估状态的好坏，这一步用策略网络做自我博弈，用<strong>游戏胜负结果作为Target，</strong>让<strong>价值网络</strong>来拟合这个Target。</p> <blockquote> <p>注意：<br> （1）behavior cloning不是强化学习，是一种监督学习。<br> （2）AlphaGo的策略网络和价值网络不是同时训练，不属于Actor-Critic算法。</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-13fa638ff26fe8652b77beb45f0692e2_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-13fa638ff26fe8652b77beb45f0692e2_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-13fa638ff26fe8652b77beb45f0692e2_1440w.jpg></a></p> <p>当完成上述训练后，AlphaGo完全可以使用策略网络进行下棋，但更好的方式是采用<strong>蒙特卡洛树搜索（Monte Carlo Tree Search）</strong>。MCTS 不需要训练，可以根据前面所训练的训练策略网络和价值网络直接做决策。</p> <h3 id=_7>二、蒙特卡洛树搜索主要思想<a class=headerlink href=#_7 title="Permanent link">¶</a></h3> <p>人类玩家在下棋的时候通常都会向前看几步，越是高手，看的越远。MCTS 的基本原理就是向前看，通过每一轮的成千上万次的模拟，从而找出当前最优的动作。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-0525d315f36a89b2cc70d259199489f8_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-0525d315f36a89b2cc70d259199489f8_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-0525d315f36a89b2cc70d259199489f8_1440w.jpg></a></p> <h3 id=_8>三、蒙特卡洛树搜索四步骤<a class=headerlink href=#_8 title="Permanent link">¶</a></h3> <p>MCTS每一轮模拟都分为选择（selection）、扩展（expansion）、求值（evaluation）、回溯（backup）四部分。AlphaGo每走一步棋都要重复上面这些步骤，通过成<strong>千上万次模拟</strong>，AlphaGo便有了每个动作的 <span class=arithmatex>\(Q(a)\)</span> 分数与 <span class=arithmatex>\(N(a)\)</span> 分数，然后AlphaGo会<strong>选择最大的<span class=arithmatex>\(Q(a)\)</span> 分数对应的动作</strong>来执行。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-2c02b30567f2480d26f609c76c3f3db6_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-2c02b30567f2480d26f609c76c3f3db6_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-2c02b30567f2480d26f609c76c3f3db6_1440w.jpg></a></p> <p><strong>（1）选择（Selection）：</strong>选择的目的是找出胜算较高的动作，忽略掉其它不好的动作。首先它将所有动作都打上分，好的动作分数高，坏的动作分数低：</p> <div class=arithmatex>\[ score(a) = Q(a) + \frac{\eta}{1+N(a)} \cdot \pi(a \mid s;\theta) \]</div> <blockquote> <p>注意：<br> （1）<span class=arithmatex>\(Q(a)\)</span> 是搜索计算出来的分数，称为动作价值，主要由胜率和价值函数决定。<span class=arithmatex>\(Q(a)\)</span>实际上是个表，记录了361个动作的分数。它的的初始值是 0，动作 a 每被选中一次，就会更新一次<span class=arithmatex>\(Q(a)\)</span> 。<br> （2）<span class=arithmatex>\(N(a)\)</span> 是动作 a 选中的次数，用于避免统一动作被探索很多次<br> （3）<span class=arithmatex>\(\eta\)</span> 是一个需要调的超参数，需要手动调整</p> </blockquote> <p>例如在这个例子中，选中了分数最高的0.8</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-5920dc8efc62232d6840f936fcf4ebd3_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-5920dc8efc62232d6840f936fcf4ebd3_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-5920dc8efc62232d6840f936fcf4ebd3_1440w.jpg></a></p> <p><strong>（2）扩展（Expansion）：</strong> 把第一步选中的动作记作<span class=arithmatex>\(a_t\)</span> ，它只是个<strong>假想的动作</strong>，而不是 AlphaGo 真正执行的动作。然后通过策略函数来模拟对手，通过策略网络和<strong>对手视角下</strong>的状态 <span class=arithmatex>\(s'_t\)</span> 随机抽样一个动作 <span class=arithmatex>\(a'_t\)</span> ，并产生新的状态 <span class=arithmatex>\(s'_{t+1}\)</span> 。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-e5a27c144e5171affa2554b3ee39ed92_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-e5a27c144e5171affa2554b3ee39ed92_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-e5a27c144e5171affa2554b3ee39ed92_1440w.jpg></a></p> <blockquote> <p>注意：此阶段会模拟对手所有下子的情况，因此是随机抽样。</p> </blockquote> <p><strong>（3）求值（evaluation）：</strong></p> <p>这部分采用两种方式来评价状态<span class=arithmatex>\(s'_{t+1}\)</span> 好坏：</p> <p>（1）使用Fast Rollout评价：从让策略网络<strong>自我博弈</strong>，直到分出胜负为止，然后后输出奖励 <span class=arithmatex>\(r_r\)</span> 来评价状态<span class=arithmatex>\(s'_{t+1}\)</span>的好坏。</p> <p>（2）使用价值网络评价：使用前面训练出来的<strong>价值网络</strong> <span class=arithmatex>\(v(s_{t+1},w)\)</span> 来评价。</p> <p>然后将两个分数求平均，作为状态<span class=arithmatex>\(s'_{t+1}\)</span>的分数：</p> <div class=arithmatex>\[ V(s_{t+1}) = \frac{1}{2} v(s_{t+1}; \mathbf{w}) + \frac{1}{2} r_T \]</div> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-df64d08f1b38f9a85162306b5a82f32e_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-df64d08f1b38f9a85162306b5a82f32e_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-df64d08f1b38f9a85162306b5a82f32e_1440w.jpg></a></p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-35ae24b79a826d385c4c1b3ee44a0065_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-35ae24b79a826d385c4c1b3ee44a0065_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-35ae24b79a826d385c4c1b3ee44a0065_1440w.jpg></a></p> <p><strong>（4）回溯（backup）：</strong></p> <p>将假想的动作 <span class=arithmatex>\(a_t\)</span> 下所有的分数作为<span class=arithmatex>\(a_t\)</span>新的价值 <span class=arithmatex>\(Q(a_t)\)</span> ，AlphaGo的决策就是选中最大<span class=arithmatex>\(Q(a_t)\)</span>值。</p> <div class=arithmatex>\[ Q(a_t) = \text{mean}(\text{the recorded } V's) \]</div> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-634301fd0fb395659a1dc8cccf927f87_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-634301fd0fb395659a1dc8cccf927f87_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-634301fd0fb395659a1dc8cccf927f87_1440w.jpg></a></p> <h3 id=alphago-zero-alphago>四、AlphaGo Zero 与 AlphaGo<a class=headerlink href=#alphago-zero-alphago title="Permanent link">¶</a></h3> <p>最新版的AlphaGo Zero 与前面的版本有所区别：一，不再使用人类的数据进行训练；二；MCTS被用来训练策略网络。结果两者相比，新版AlphaGo Zero完败旧版AlphaGo 。</p> <blockquote> <p>注意：这个例子不能说明behavior cloning没有用，在其他物理世界模拟中，如自动驾驶、手术等等，behavior cloningkey有效的减少最坏的情况，不需要撞坏十几万辆车、不需要给真人动十几万次手术。</p> </blockquote> <p>最新版的AlphaGo Zero训练步骤如下：</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-2ee6f40cddc9b4159f4c4cc2cba423ec_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-2ee6f40cddc9b4159f4c4cc2cba423ec_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-2ee6f40cddc9b4159f4c4cc2cba423ec_1440w.jpg></a></p> <hr> <h2 id=p6>P6 蒙特卡罗方法<a class=headerlink href=#p6 title="Permanent link">¶</a></h2> <h3 id=_9>一、 蒙特卡罗方法算法简介<a class=headerlink href=#_9 title="Permanent link">¶</a></h3> <p>蒙特卡罗名字来源于<strong>摩纳哥的梦塔卡罗赌场，</strong>算法的名字来源于统计学大师Nicholas Metropolis1947年论文。蒙特卡罗方法的定义如下：</p> <blockquote> <p>蒙特卡罗是一种数值算法，靠重复随机样本（repeated random sampling）来对目标做近似。</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-bbb416f7e7510e498796b9c824740fd3_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-bbb416f7e7510e498796b9c824740fd3_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-bbb416f7e7510e498796b9c824740fd3_1440w.jpg></a></p> <blockquote> <p>注意：蒙特卡罗的算法往往是错的，只是目标的近似</p> </blockquote> <p>下面是个实际的例子，采用蒙特卡罗方法计算某个复杂函数的定积分。首先我们在区间 <span class=arithmatex>\([0.8,3]\)</span> 随机抽样得到 <span class=arithmatex>\(n\)</span> 个样本，记作 <span class=arithmatex>\(x_1,...,x_n\)</span> ，然后计算每个样本的函数值并取平均再乘以区间大小2.2，结果记作 <span class=arithmatex>\(Q_n\)</span> ，最后输出<span class=arithmatex>\(Q_n\)</span>作为定积分的近似。 </p> <blockquote> <p>注意：根据大数定理可以保证，当 <span class=arithmatex>\(n\)</span> 区域无穷时，<span class=arithmatex>\(Q_n\)</span>的值趋于 <span class=arithmatex>\(I\)</span></p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-c98f34b717e44c6034a3e37d342b463a_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-c98f34b717e44c6034a3e37d342b463a_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-c98f34b717e44c6034a3e37d342b463a_1440w.jpg></a></p> <h3 id=_10>二、时间差分学习与蒙特卡罗方法对比<a class=headerlink href=#_10 title="Permanent link">¶</a></h3> <table> <thead> <tr> <th>特性</th> <th>时间差分学习 (TD Learning)</th> <th>蒙特卡罗方法 (Monte Carlo Methods)</th> </tr> </thead> <tbody> <tr> <td>更新时机</td> <td>在线更新，每一步都会更新价值函数。</td> <td>在整个状态或动作序列结束后进行更新。</td> </tr> <tr> <td>关键思想</td> <td>用下一个状态的价值来估计当前状态的价值，并据此进行更新。</td> <td>基于采样来估计价值函数，使用完整的序列信息</td> </tr> <tr> <td>优点</td> <td>在线学习，无需等待整个序列结束，具有较高的样本效率</td> <td>精确度高，因为它使用了完整的序列信息</td> </tr> <tr> <td>缺点</td> <td>可能不够精确，因为它依赖于其他状态的价值估计</td> <td>需要等待整个序列完成，因此在计算和时间上可能较慢</td> </tr> <tr> <td>计算效率</td> <td>较高</td> <td>较低</td> </tr> <tr> <td>准确性</td> <td>较低</td> <td>较高</td> </tr> <tr> <td>适用场景</td> <td>动态和未知的场景</td> <td>状态转换和奖励非常确定的场景</td> </tr> </tbody> </table> <p>参考资料：</p> <p><a href=https://zhuanlan.zhihu.com/p/658564004>Native8418：什么是时间差分（Temporal Difference）学习，它如何与蒙特卡罗方法相比？</a></p> <hr> <h2 id=p7-fisher-yates>P7 随机排列与Fisher-Yates算法<a class=headerlink href=#p7-fisher-yates title="Permanent link">¶</a></h2> <h3 id=_11>一、随机排列问题<a class=headerlink href=#_11 title="Permanent link">¶</a></h3> <p>1.均匀随机排列数学定义：把n个元素变成一个随机序列，序列中的位置都是随机的</p> <p>2.三种随机排列定义</p> <p>（1）从所有可能的序列中做随机抽样得到一个序列，一共有 $n！ $种可能的序列，抽到的序列是所有序列中的一个</p> <p>（2）排列以后，一个元素可以在 <span class=arithmatex>\(\left\{ 0,1,...,n-1 \right\}\)</span> 之间的任何一个位置上，且概率都是 <span class=arithmatex>\(1/n\)</span> 。</p> <p>（3）排列以后，一个位置上出现任何元素，且他们的概率相等。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-d82519346e87289d7152dc109efc884b_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-d82519346e87289d7152dc109efc884b_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-d82519346e87289d7152dc109efc884b_1440w.jpg></a></p> <h3 id=fisher-yates>二、Fisher-Yates算法<a class=headerlink href=#fisher-yates title="Permanent link">¶</a></h3> <p>基本想法：在<strong>第i次</strong>循环中，我们在剩余的元素中做<strong>随机抽样</strong>选取一个元素并把它放到<strong>第i号</strong>位置中。</p> <p>特点：时间复杂度： <span class=arithmatex>\(O(n^2)\)</span> ，且需要额外数组特点</p> <h3 id=fisher-yates_1>三、改进版Fisher-Yates算法过程<a class=headerlink href=#fisher-yates_1 title="Permanent link">¶</a></h3> <p><strong>（1）随机抽取，然后将选中的数字放到第i轮对应的各子</strong></p> <p>特点：时间复杂度： <span class=arithmatex>\(O(n)\)</span> ，且直接操作数组。两者区别如下：</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-4a9fb0f1e7e526a94b4d6f36e4f78e6a_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-4a9fb0f1e7e526a94b4d6f36e4f78e6a_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-4a9fb0f1e7e526a94b4d6f36e4f78e6a_1440w.jpg></a></p> <hr> <h2 id=p8-sarsa>P8 Sarsa算法<a class=headerlink href=#p8-sarsa title="Permanent link">¶</a></h2> <p>Sarsa（state action reward state action），算法的目的是<strong>学习动作价值函数 <span class=arithmatex>\(Q(\pi)\)</span></strong> ，其可以用在表格形式的强化学习，直接去学<span class=arithmatex>\(Q(\pi)\)</span>函数，但前提是状态和动作的数量数有限的，sarsa算法每次更新表格中的一个元素，T让TD error减小；也可以用来学习价值网络，Sarsa算法每次用一个五元组来更新参数w（实际上在前面的actor-critic算法课中，我们就用了sarsa算法更新价值网络参数w）</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-fff47efb93655272a4fdf9680b5d8cff_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-fff47efb93655272a4fdf9680b5d8cff_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-fff47efb93655272a4fdf9680b5d8cff_1440w.jpg></a></p> <h3 id=sarsatabular-version>一、Sarsa(Tabular Version)<a class=headerlink href=#sarsatabular-version title="Permanent link">¶</a></h3> <p>适用情形：状态和动作的数量有限</p> <h3 id=sarsavalue-network-version>二、Sarsa(Value Network Version)<a class=headerlink href=#sarsavalue-network-version title="Permanent link">¶</a></h3> <p>适用情形：</p> <hr> <h2 id=p9-q-learning>P9 Q-Learning算法<a class=headerlink href=#p9-q-learning title="Permanent link">¶</a></h2> <p>与Sarsa算法不同，Q-Learning的目标是学<strong>习最优动作价值函数</strong> <span class=arithmatex>\(Q^*\)</span> ，其可以用在表格上直接去学习<span class=arithmatex>\(Q^*\)</span>，但前提是状态和动作的数量数有限的，Q-Learning算法每次更新表格中的一个元素，让TD error减小；也可以用来训练DQN，Q-Learning算法每次用一个观测到的transition来更新一次参数w。</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-34619346708ba7a4dd18d994e7f500ef_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-34619346708ba7a4dd18d994e7f500ef_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-34619346708ba7a4dd18d994e7f500ef_1440w.jpg></a></p> <h3 id=q-learningtabular-version>一、Q-Learning(Tabular Version)<a class=headerlink href=#q-learningtabular-version title="Permanent link">¶</a></h3> <h3 id=q-learningdqn-version>二、Q-Learning(DQN Version)<a class=headerlink href=#q-learningdqn-version title="Permanent link">¶</a></h3> <h3 id=sarsa-q-learning>三、Sarsa 与 Q-Learning<a class=headerlink href=#sarsa-q-learning title="Permanent link">¶</a></h3> <blockquote> <p>注意：两种算法的TD target都只包含一个奖励，这是标准的TD target，如果是多个奖励，那就变成我们下面介绍的Multi-Step TD Target。</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-b7abcdf4794a7d7624f82a1230ee9949_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-b7abcdf4794a7d7624f82a1230ee9949_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-b7abcdf4794a7d7624f82a1230ee9949_1440w.jpg></a></p> <hr> <h2 id=p10-multi-step-td-target>P10 Multi-Step TD Target<a class=headerlink href=#p10-multi-step-td-target title="Permanent link">¶</a></h2> <p>是TD算法的改进</p> <hr> <h2 id=p11-experience-replay>P11 经验回放 Experience Replay<a class=headerlink href=#p11-experience-replay title="Permanent link">¶</a></h2> <p>经验回放作用： 既可以重复利用经验，避免浪费；也可以把序列打散，消除相关性</p> <h3 id=experience-replay>一、Experience Replay<a class=headerlink href=#experience-replay title="Permanent link">¶</a></h3> <h3 id=prioritized-experience-replay>二、Prioritized Experience Replay<a class=headerlink href=#prioritized-experience-replay title="Permanent link">¶</a></h3> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-5b02cc3643d6280aeed64f54844f553d_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-5b02cc3643d6280aeed64f54844f553d_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-5b02cc3643d6280aeed64f54844f553d_1440w.jpg></a></p> <hr> <h2 id=p12-target-networkdouble-d>P12 高估问题、Target Network、Double D<a class=headerlink href=#p12-target-networkdouble-d title="Permanent link">¶</a></h2> <h3 id=_12>一、高估问题<a class=headerlink href=#_12 title="Permanent link">¶</a></h3> <p>TD算法导致DQN高估真实动作价值的两个原因：</p> <p>（1）计算TD target的时候用到了<strong>最大化</strong></p> <p>（2）<strong>bootstrapping（自举）</strong>，用自己的估计更新自己</p> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-2d06e5c754dacdfa24a989975bee7506_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-2d06e5c754dacdfa24a989975bee7506_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-2d06e5c754dacdfa24a989975bee7506_1440w.jpg></a></p> <h3 id=target-network>二、Target Network<a class=headerlink href=#target-network title="Permanent link">¶</a></h3> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-ba993ffb12b83fd73b2a5f8ee757afae_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-ba993ffb12b83fd73b2a5f8ee757afae_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-ba993ffb12b83fd73b2a5f8ee757afae_1440w.jpg></a></p> <h3 id=double-d>三、Double D<a class=headerlink href=#double-d title="Permanent link">¶</a></h3> <h3 id=td-target>四、三种TD target计算方式对比<a class=headerlink href=#td-target title="Permanent link">¶</a></h3> <p><a class=glightbox href=https://raw.githubusercontent.com/Tendourisu/images/master/v2-a8214ec538580c7174ebc4c9db2705b1_1440w.jpg data-type=image data-width=80% data-height=auto data-desc-position=bottom><img alt=v2-a8214ec538580c7174ebc4c9db2705b1_1440w.jpg src=https://raw.githubusercontent.com/Tendourisu/images/master/v2-a8214ec538580c7174ebc4c9db2705b1_1440w.jpg></a></p></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="February 28, 2025 15:44:38">February 28, 2025 15:44:38</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="February 28, 2025 15:44:38">February 28, 2025 15:44:38</span> </span> </aside> <!-- Insert generated snippet here --> <script src=https://giscus.app/client.js data-repo=Tendourisu/Tendourisu.github.io data-repo-id=R_kgDOMhyGaQ data-category=Announcements data-category-id=DIC_kwDOMhyGac4Cnbam data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async>
    </script> <!-- Synchronize Giscus theme with palette --> <script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate"
            ? "transparent_dark"
            : "light"

        // Instruct Giscus to set theme
        giscus.setAttribute("data-theme", theme)
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function () {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function () {
            var palette = __md_get("__palette")
            if (palette && typeof palette.color === "object") {
                var theme = palette.color.scheme === "slate"
                    ? "transparent_dark"
                    : "light"

                // Instruct Giscus to change theme
                var frame = document.querySelector(".giscus-frame")
                frame.contentWindow.postMessage(
                    { giscus: { setConfig: { theme } } },
                    "https://giscus.app"
                )
            }
        })
    })
</script> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 - Present <a href=https://github.com/Tendourisu/ target=_blank rel=noopener>Tendourisu</a> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/Tendourisu/ target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=/img/qq.jpg target=_blank rel=noopener title=加加我的QQ class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433.754 420.445c-11.526 1.393-44.86-52.741-44.86-52.741 0 31.345-16.136 72.247-51.051 101.786 16.842 5.192 54.843 19.167 45.803 34.421-7.316 12.343-125.51 7.881-159.632 4.037-34.122 3.844-152.316 8.306-159.632-4.037-9.045-15.25 28.918-29.214 45.783-34.415-34.92-29.539-51.059-70.445-51.059-101.792 0 0-33.334 54.134-44.859 52.741-5.37-.65-12.424-29.644 9.347-99.704 10.261-33.024 21.995-60.478 40.144-105.779C60.683 98.063 108.982.006 224 0c113.737.006 163.156 96.133 160.264 214.963 18.118 45.223 29.912 72.85 40.144 105.778 21.768 70.06 14.716 99.053 9.346 99.704"/></svg> </a> <a href=/img/wechat.jpg target=_blank rel=noopener title=加加我的微信 class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6"/></svg> </a> <a href=https://x.com/tendourisu target=_blank rel=noopener title=X class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> <a href=https://www.zhihu.com/people/tendourisu target=_blank rel=noopener title=Zhihu class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg> </a> <a href=mailto:2681708668@qq.com target=_blank rel=noopener title="send email to me!" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "header.autohide", "navigation.tracking", "navigation.tabs", "navigation.top", "navigation.path", "navigation.indexes", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script> <script src=../../../js/mathjax.js></script> <script src=../../../js/toc.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>