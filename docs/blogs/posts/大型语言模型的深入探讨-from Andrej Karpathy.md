---
title: 大型语言模型的深入探讨-from Andrej Karpathy
tags:
  - LLM
categories: 
date: 2025-03-14T21:38:55+08:00
modify: 2025-03-14T21:38:55+08:00
dir: 
share: false
cdate: 2025-03-14
mdate: 2025-03-14
---

# 大型语言模型（LLMs）如 ChatGPT 的深入探讨
> [!meta]+
> youtube: [Deep Dive into LLMs like ChatGPT - YouTube](https://www.youtube.com/watch?v=7xTGNNLPyMI)
> bilibili: [【1080P】安德烈·卡帕西：深入探索像ChatGPT这样的大语言模型｜Andrej Karpathy\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV16cNEeXEer)

## 构建 ChatGPT 的过程

> 整个流程分为多个 **顺序阶段**。

### 第一阶段：预训练（Pre-training）

> **第一步：下载和处理互联网数据**。
>
> - **目标是获取大量高质量和多样性的互联网公开文本数据**，以便模型学习广泛的知识。
> - Hugging Face 的 **FineWeb 数据集** 是一个代表性的例子，大小约为 **44 TB**。
> - 尽管互联网数据量巨大，但经过 **过滤** 后，最终用于训练的文本数据量会减少。
> - **Common Crawl** 是这些努力的起点，自 2007 年以来一直在抓取互联网数据，截至 2024 年已索引 **27 亿个网页**。
> - Common Crawl 的数据非常原始，需要经过多重 **过滤**。

#### 过滤阶段的步骤

>
>
> - **URL 过滤：** 移除恶意软件、垃圾邮件、营销、种族主义、成人等不良网站的 URL。
> - **文本提取：** 从原始 HTML 网页中提取纯文本内容，去除标记、导航等无用信息。
> - **语言过滤：** 识别网页语言，例如 FineWeb 过滤掉英语占比低于 65% 的网页。这会影响模型后续在不同语言上的表现。
> - **重复数据删除（Deduplication）**。
> - **个人身份信息（PII）移除：** 检测并过滤包含地址、社会安全号码等个人信息的网页。

> 预处理后得到最终的训练文本数据，例如 FineWeb 数据集。
>
> 将所有文本 **拼接** 成一个巨大的 **一维文本序列**。
>
> 目标是训练神经网络来 **模仿和建模** 这种文本的流动模式。

## 将文本输入神经网络

> 神经网络期望输入 **一维的符号序列**，并且符号集是有限的。
>
> 需要决定 **什么是符号**，以及如何将文本表示为这些符号的序列。
>
> 当前的文本是一个一维序列，但计算机需要底层的二进制表示 (UTF8 编码)。
>
> **序列长度** 是神经网络中宝贵且有限的资源。
>
> 需要在 **词汇表大小（符号数量）和序列长度** 之间进行权衡：不希望只有少量符号但序列极长，而是希望有更多符号和更短的序列。

### 编码方式

>
>
> - **字节（Byte）编码：** 一种简单的压缩方式，将连续的 8 位（bits）组合成一个字节。8 位有 256 种可能的组合，因此可以将文本表示为字节序列，序列长度缩短为原来的八分之一，但符号数量增加到 256。可以将每个字节想象成一个独特的 ID 或 Emoji。
> - **字节对编码（Byte Pair Encoding - BPE）：** 一种更先进的技术，通过查找并合并频繁出现的连续字节或符号对来进一步缩短序列长度并增加词汇表大小。
>   - 例如，频繁出现的字节序列 116 和 32（对应 "t" 和空格）可以合并为一个新的符号，ID 为 256。
>   - 这个过程可以迭代进行，每次合并都会减少序列长度并增加词汇表大小。
>   - 实践中，一个好的词汇表大小约为 **10 万个符号**。
>   - **GPT-4 使用了 100,277 个符号**。
> - **分词（Tokenization）：** 将原始文本转换为这些符号（或称为 **tokens** ）的过程。

> 可以使用 **Tick Tokenizer** 网站探索不同模型的 tokenizer 如何工作，例如 GPT-4 使用的 **CL100K\_base**。
>
> 分词是 **大小写敏感** 的，标点符号和空格也会影响分词结果。

## 模型训练与推理

> 一旦拥有训练好的神经网络和一组特定的参数（权重），就可以使用该模型进行 **推理（Inference）**，即生成数据。
>
> 当与 ChatGPT 等模型交互时，你所对话的模型是 **预先训练好的**，其参数是固定的，对话过程只是 **推理**，没有进一步的训练发生。
>
> 模型通过接收你输入的 token 序列，然后 **预测并补全** 后续的 token 序列来生成回复。

## GPT-2 模型的例子

> **GPT（Generatively Pre-trained Transformer）**是由 OpenAI 开发的一系列生成式预训练 Transformer 模型。
>
> **GPT-2** 是该系列的第二个版本，于 2019 年发布，标志着现代 LLM 技术栈的首次完整结合。
>
> 虽然 GPT-2 的所有组件在现代标准看来都相对较小，但其基本架构与今天的模型类似。
>
> **GPT-2 的关键参数：**
>
> - **16 亿参数** （相比之下，现代模型可能接近万亿或数千亿）
> - **1024 个 token 的最大上下文长度** （现代模型的上下文长度可达数十万甚至百万）
> - **约 1000 亿个 token 的训练数据量** （相比之下，FineWeb 数据集有 15 万亿个 token）

> 重新复现 GPT-2 的成本在 2019 年估计约为 4 万美元，如今由于硬件和软件的进步，成本已大大降低。
>
> 数据质量的提高和计算硬件及软件的进步是成本降低的主要原因。

## 模型训练的计算资源

> 训练大型语言模型需要大量的计算资源，通常使用 **GPU（图形处理器）**集群。
>
> 例如，可以使用包含 **8 个 NVIDIA H100 GPU** 的节点进行训练。
>
> 这些计算机通常在云端租用，例如 Lambda 等提供商。
>
> 单个 NVIDIA H100 GPU 本身非常昂贵，按需租赁的价格约为每 GPU 每小时 3 美元。
>
> GPU 非常适合神经网络训练，因为它们能进行 **大规模并行计算**，高效处理模型训练中涉及的矩阵乘法。
>
> 多个 GPU 可以组合成一个节点，多个节点可以组成一个数据中心，形成庞大的计算系统。
>
> 对 GPU 的巨大需求是推动 **NVIDIA 股价飙升** 的关键因素。
>
> 所有这些计算资源的目标都是 **预测下一个 token**，从而不断改进神经网络。

## 基础模型（Base Models）与助手模型（Assistant Models）

> 大型科技公司会定期训练这些模型，并在训练完成后发布一些。
>
> **基础模型** 是训练的直接产物，本质上是一个 **token 模拟器** 或 **互联网文本 token 模拟器**。
>
> 基础模型本身并不直接有用，因为它只是生成互联网文本的“混音”，像是在“做互联网的梦”，无法回答问题或提供帮助。
>
> 我们真正想要的是 **助手模型**，能够回答问题并进行交互。
>
> 尽管如此，一些公司还是会发布基础模型。
>
> **模型发布通常包括两部分：**
>
> - **描述模型操作的 Python 代码**，即神经网络的前向传播过程。这通常只有几百行代码。
> - **模型的参数（权重）**，即训练得到的数十亿个数字，代表了模型的“知识”。这是模型价值的核心所在。

> **GPT-2** 是一个发布的基础模型（15 亿参数，1000 亿 token 训练）。
>
> **Llama 3** 是 Meta 发布的一个更现代、更大的基础模型（4050 亿参数，15 万亿 token 训练）。
>
> 可以通过像 **Hyperbolic** 这样的平台与 Llama 3 等基础模型进行交互。
>
> **基础模型还不是助手**，不能像 ChatGPT 那样回答问题，它们只是根据训练数据的统计规律补全 token 序列。
>
> 基础模型的输出是 **随机的（stochastic）**，对于相同的输入，每次可能会产生不同的结果，因为模型会从预测的下一个 token 的概率分布中进行采样。
>
> 尽管基础模型本身用途有限，但它们在 **预测下一个 token 的任务中学习了大量关于世界的知识**，这些知识存储在网络的参数中。
>
> 可以将这些参数视为互联网数据的某种 **有损压缩**。
>
> 可以通过 **巧妙的提示（Prompting）来激发基础模型中隐藏的知识**。
>
> - 例如，通过提供一个“Top 10 list”的开头，可以引导模型继续生成相关内容。

> 基础模型可能会出现 **记忆训练数据的现象（Regurgitation）**，特别是对于像维基百科这样高质量且频繁出现的数据。
>
> 基础模型对于 **训练数据截止日期之后的信息一无所知**，如果提示包含未来信息，模型会根据其现有知识进行 **猜测（Hallucination）**。
>
> 即使是基础模型，也可以通过 **少样本提示（Few-shot Prompting）应用于实际应用，利用模型的上下文学习能力（In-context Learning）**，使其学习并遵循提示中的模式。
>
> - 例如，通过提供一些英语-韩语的翻译示例，可以让模型在上下文中学习并进行翻译。

> 也可以通过构建类似 **人与 AI 助手对话网页的提示**，让基础模型扮演助手的角色并继续对话。

## 将基础模型转化为助手模型：指令微调（Instruction Fine-tuning）

> 要将基础模型转化为能够理解指令并进行对话的助手模型，需要进行 **指令微调**。
>
> 这个过程仍然是 **继续训练** 基础模型，但使用的是 **高质量的人类对话数据集**。
>
> 这个数据集远小于预训练数据集，因此训练时间相对较短。
>
> 关键在于如何将 **对话** 表示为模型可以理解的 **token 序列**。
>
> 需要设计 **编码规则和协议**，类似于互联网的 TCP/IP 协议，将对话结构化信息编码和解码为 token。
>
> 例如，一个用户和助手之间的两轮对话可以编码成一个 **一维的 token 序列**。
>
> 不同的 LLM 可能有不同的对话格式和协议，目前这方面还比较混乱。
>
> 例如，GPT-4 使用特殊的 token，如 `<|im_start|>`（想象的独白开始）来标记用户或助手的回合，以及问题或回答的开始和结束。
>
> 这些特殊 token 是在 **后训练阶段** 引入的 **新 token**，模型需要学习它们的含义。
>
> 通过在大量这样的对话数据上进行训练，模型可以学习理解对话的结构和不同角色的预期行为。
>
> 在 **推理时**，当用户输入新的对话轮次时，系统会在后台构建包含上下文的 token 序列，并指示模型 **继续生成** 助手的回复 token。

## 构建高质量的对话数据集

> OpenAI 在 2022 年的 **InstructGPT 论文** 中首次公开讨论了如何通过在对话数据上微调语言模型。
>
> 关键在于构建 **高质量的对话数据集**，这通常涉及到 **人工标注员**。
>
> 人工标注员（通常通过 Upwork 或 Scale AI 等平台雇佣）的任务是 **创建提示（Prompts）**，并 **编写理想的助手回复**。
>
> 标注员需要遵循公司提供的 **标注指南**，这些指南通常要求回复是 **有帮助的（helpful）、真实的（truthful）和无害的（harmless）**。这些指南可能非常详细，长达数百页。
>
> 这是一个 **人工密集型** 的过程。
>
> 也有一些 **开源项目** 试图通过互联网众包的方式收集类似的对话数据，例如 **Open Assistant**。
>
> 通过在这些示例对话上进行训练，模型会逐渐学习并 **采纳助手的人格**。
>
> 尽管训练数据无法覆盖所有可能的提问，但模型会学习 **统计模式**，并在遇到类似问题时给出符合预期风格的回复。
>
> 近年来，构建对话数据集的方式有所发展，不再完全依赖人工标注。

### 使用语言模型自身生成和评估数据

> 现在更常见的是 **利用大型语言模型自身来生成候选的助手回复**。
>
> 然后，可以使用 **另一个（可能更强大或专门训练过的）语言模型来评估这些候选回复的质量**，并选择最佳的作为训练数据。
>
> 这个过程可以 **自动化**，大大减少了对大量人工标注的依赖。
>
> 最终目标是训练出一个模型，其行为能够 **统计性地模仿人类标注员**，而这些标注员又是根据公司编写的标注指南工作的。
>
> 与 ChatGPT 等助手模型的交互，可以被视为与一个 **模拟的人类标注员** 进行对话，这个“标注员”可能具有相当高的专业技能。
>
> 当你向 ChatGPT 提出问题时，得到的回复并非来自一个拥有无限智慧的“魔法 AI”，而是一个 **根据 OpenAI 雇佣的标注员的行为进行统计模拟的模型**。如果你的问题与训练数据中的某个示例非常相似，你很可能会得到类似的答案。

## 缓解幻觉问题（Mitigating Hallucinations）

> 早期的语言模型容易 **生成不真实或编造的信息，即幻觉（Hallucinations）**。
>
> 例如，当被问及一个模型不知道的虚构人物时，它可能会编造出看似合理的答案。
>
> 要解决这个问题，需要在训练数据中包含 **模型不知道某些事实时的正确答案**，即模型应该回答“我不知道”。
>
> 关键在于 **如何判断模型知道什么和不知道什么**。
>
> 可以通过 **经验性地探查模型** 来确定其知识边界。
>
> Meta 在 Llama 3 系列模型中就采取了这样的方法，**询问模型关于训练数据中随机文档的问题**，以确定其知识范围，并添加模型回答“我不知道”的示例到训练集中。
>
> 这之所以有效，是因为模型内部可能已经存在关于自身知识状态的表征（例如，某个神经元在模型不确定时激活），但需要 **将其激活与“我不知道”的语言表达连接起来**。
>
> Meta 的方法包括 **使用另一个 LLM 基于训练数据中的段落生成问题和答案**，然后利用这些信息来训练模型在不知道答案时说“我不知道”。

## 工具使用（Tool Use）

> 另一种缓解幻觉并扩展模型能力的方法是 **让模型能够使用外部工具**。
>
> 例如，当模型不知道某个问题的答案时，它可以 **使用搜索引擎（如 Bing 或 Google）来查找信息**。
>
> 这通过引入 **特殊 token** 来实现，例如 `<search_start>` 和 `<search_end>`，模型可以生成包含搜索查询的这些 token。
>
> 当推理程序看到 `<search_end>` token 时，会 **暂停生成**，并将查询发送给搜索引擎，然后将搜索结果 **插入到模型的上下文窗口中**。
>
> 模型现在可以 **基于搜索结果生成更准确的答案**，甚至可以引用来源。
>
> ChatGPT 等模型已经具备这种工具使用能力，当用户提出需要最新信息的问题时，模型可能会自动进行网页搜索。
>
> 用户也可以明确指示模型不要使用工具，只依赖其内部记忆。
>
> 工具使用还可以扩展到其他领域，例如 **代码执行**。模型可以生成代码，然后使用解释器（如 Python 解释器）运行代码，并根据结果给出答案，这对于数学计算等任务非常有用，可以提高准确性。
>
> **神经网络参数中的知识是一种模糊的回忆，而上下文窗口中的 token 则是工作记忆**。
>
> 当需要模型回忆特定信息时，**直接将相关文本放入上下文窗口通常比仅仅依赖模型的记忆效果更好**。

## 模型认知的怪异之处

### 模型自我认知（Knowledge of Self）

> **模型如何识别自己？** 例如，ChatGPT 会说自己是由 OpenAI 开发的。
>
> 这通常是因为训练数据中包含大量类似的回应。
>
> 开发者可以通过 **硬编码训练数据** 或使用**系统消息（System Message）**在对话开始时提醒模型其身份信息来覆盖这种默认行为。
>
> 这些自我认知信息本质上是 **后天添加的**，并非模型深层固有的。

### 计算能力（Computational Capabilities）

> 模型在解决问题时需要以 token 为单位逐步进行计算。
>
> **每个 token 的计算量是有限的**，因此模型无法在单个 token 中完成复杂的计算。
>
> 解决需要多步计算的问题时，应该 **鼓励模型将推理和计算分布在多个 token 上**，逐步得出中间结果。
>
> 直接要求模型在单个 token 中给出复杂问题的答案通常行不通。
>
> OpenAI 等公司在训练数据标注时会考虑到这一点，确保答案的生成过程包含中间推理步骤。
>
> 可以“强迫”模型尝试在单个 token 中回答问题，但结果通常不可靠。
>
> 对于复杂的计算任务，**最好让模型使用代码解释器等工具**，而不是依赖其“心算”能力。

### 计数（Counting）

> 模型由于每个 token 的计算限制和不直接处理字符，通常 **不擅长精确计数**。
>
> 例如，让模型数上下有多少个点，它可能会给出错误的答案。
>
> 解决计数问题时，**应该让模型使用代码工具**，将需要计数的内容复制到代码中，并利用代码的计数功能。

### 拼写相关任务（Spelling）

> 模型 **不擅长** 拼写相关任务，因为它处理的是 token 而不是单个字符。
>
> 即使是看似简单的任务，例如提取字符串中每隔一个字符，模型也可能失败。
>
> 这是因为模型需要理解 token 和字符之间的关系，而 token 的设计主要是为了效率。
>
> 未来可能会出现直接基于字符或字节的模型，但这会带来序列长度过长的问题。
>
> 对于拼写相关任务， **应该再次利用代码工具**。
>
> “草莓里有多少个 R？”这个问题曾经难倒很多模型，原因也是模型不直接处理字符且不擅长计数。
>
> 总而言之，在使用这些模型时，需要了解它们的这些“认知缺陷”，并在必要时借助工具来弥补。

## 强化学习（Reinforcement Learning）

> 强化学习（RL）是训练大型语言模型的 **最后一个主要阶段**，通常在预训练和监督微调之后进行。
>
> 在 OpenAI 等公司中，预训练、监督微调和强化学习通常由不同的团队负责。
>
> > **动机：** 将语言模型比作学生，强化学习阶段就像让学生 **通过练习来掌握技能**。
>
> ### 与学校教育的类比
>
> >
> >
> > - **预训练：** 学习教材中的 **背景知识（Exposition）**。
> > - **监督微调（SFT）：** 学习教材中的 **例题及其专家解答（Worked Examples）**，通过模仿专家行为来学习。
> > - **强化学习（RL）：** 做教材中的 **练习题（Practice Problems）**，没有直接的专家解答，需要通过尝试和错误来发现最佳解决方案。

> 以之前的数学题为例，即使有多个都得出正确答案的解题过程，**作为人类标注员，很难判断哪个解题过程对 LLM 来说是最好的**。
>
> 人类的认知与 LLM 的认知不同，对我们来说简单的 token 序列，对 LLM 来说可能难以理解，反之亦然。
>
> 因此，仅仅依靠模仿人类专家提供的解题步骤可能不是最优的，我们希望 **LLM 能够通过试错自行发现最适合自己的解题 token 序列**。
>
> **强化学习的过程** 类似于让模型在给定的提示下生成不同的解决方案，然后根据某种**奖励信号（Reward Signal）**来判断哪个解决方案更好，并鼓励模型生成更好的解决方案。
>
> 例如，在数学问题中，奖励信号可以是答案的正确性。
>
> 通过大量的练习和奖励反馈，模型能够 **学习到更有效的问题解决策略**。
>
> 预训练和监督微调已经发展多年，是相对标准的技术，而 **强化学习在 LLM 领域的应用还处于早期阶段，并非所有机构都采用标准化流程**。
>
> 关键在于如何设计有效的奖励函数、如何探索不同的解决方案以及如何稳定训练过程，这些都涉及到许多细节和挑战。
>
> DeepSeek AI 最近发布了一篇重要的论文 **DeepSeek R1**，公开讨论了强化学习在 LLM 中的重要性以及实现有效 RL 的一些关键细节，重新激起了公众对使用 RL 改进 LLM 的兴趣。
>
> 论文展示了通过强化学习，模型在解决数学问题上的 **准确率显著提高**。
>
> 更重要的是，强化学习还改变了模型解决问题的方式，模型 **倾向于生成更长的回复**，其中包含了 **更详细的思考和推理过程**。
>
> 这表明模型正在学习 **进行更深入的思考**，并尝试不同的方法来确保答案的正确性。
>
> DeepSeek R1 模型在 **chat.deepseek.com** 上提供试用，开启“Deep Think”模式可以体验其推理能力。
>
> 对比监督微调的模型，经过强化学习训练的“推理模型”（Thinking Models）在解决复杂问题时展现出更强的“思考”过程。
>
> DeepSeek R1 是一个 **开放权重模型**，任何人都可以下载和使用。
>
> **together.ai** 等平台也托管了 DeepSeek R1 等先进的开放模型。
>
> 在 ChatGPT 中，一些标有“使用高级推理”的模型（如 01、O3 等）也是经过类似 DeepSeek R1 技术的强化学习训练的。
>
> 然而，OpenAI 在其网页界面上 **没有直接展示模型的完整思考过程**，可能是出于“蒸馏风险”的考虑，即担心他人模仿这些推理过程。
>
> 尽管如此，这些“推理模型”在性能上与 DeepSeek R1 等模型相当。
>
> 对于需要复杂推理的任务，应该尝试使用这些“推理模型”。
>
> 对于简单的知识性问题，使用普通的 GPT-4o 等模型可能就足够了，因为“推理模型”的响应时间可能会更长。
>
> Google 的 **AI Studio** 也提供了一个实验性的“思考模型” **Gemini 2.0 Flash Thinking Experimental**。
>
> Anthropic 的 **Claude** 系列模型也通过强化学习进行了训练。

## 不可验证领域中的学习

> 到目前为止讨论的强化学习都集中在 **可验证领域**，即任何候选解决方案都可以根据明确的答案进行评分（例如数学题的答案是否正确）。
>
> 可以使用 **自动化方法** 或 **LLM 判官** 来评估解决方案的质量。
>
> 但在 **不可验证领域**（例如创意写作、写笑话、写诗、摘要等），很难对不同的解决方案进行客观评分。
>
> 例如，如何判断一个关于鹈鹕的笑话是否“好”？虽然可以让人类来评估，但如果需要对大量生成的内容进行评分，这种方式效率低下。
>
> 一种方法是使用 **基于人类反馈的强化学习（Reinforcement Learning from Human Feedback - RLHF）**。
>
> 在 RLHF 中，人类标注员会对模型生成的不同回复进行 **偏好排序**，模型会学习生成人类更喜欢的回复。
>
> 但这存在一个问题，即 **奖励函数可能会被“攻破”（gameable）**。模型可能会学习生成人类认为“好”但实际上并不理想的回复，尤其是在长期迭代中。
>
> 因此，博主认为 RLHF 不算是真正的 RL，更像是一种 **微调**，可以带来一些改进，但不能指望通过无限的计算获得质的飞跃。

## 如何保持更新和查找模型

### 保持更新的资源

>
>
> - **El MaRIana Leaderboard：** 一个基于人类比较的 LLM 排行榜，可以了解哪些模型性能领先。DeepSeek 和 Llama 等开放权重模型也在榜单上。但需要注意，该榜单的可靠性近几个月有所下降。
> - **AI News Newsletter：** 由 Swyx 和朋友们维护的一个非常全面的新闻邮件，涵盖 AI 领域的最新进展。
> - **X (Twitter)：** 关注值得信赖的 AI 研究者和开发者，获取最新信息。

### 查找和使用模型

>
>
> - **专有模型：** 直接访问模型提供商的网站，例如 OpenAI (chat.openai.com)、Google (gemini.google.com 或 AI Studio)。
> - **开放权重模型：** 使用 LLM 推理服务提供商，例如 **together.ai**，它们托管了各种开放模型，包括 DeepSeek、Llama 等。
> - **基础模型：** **hyperbolic.ai** 等平台可能提供对基础模型的访问。
> - **本地运行的小型模型：** 使用 **LM Studio** 等应用程序，可以在本地计算机上运行一些较小的、经过蒸馏或低精度处理的模型。

### 总结 ChatGPT 的工作方式

>
>
> - 你的 **查询首先被切分成 token**。
> - 这些 token 根据 **对话协议格式** 构建成 token 序列。
> - ChatGPT 接收到这个 token 序列后，就像一个 **token 自动补全工具** 一样，**预测并生成** 后续的 token 序列作为回复。
> - 这个过程涉及到神经网络的计算，这是一个 **有限计算量的过程**。
> - 因此，ChatGPT 的回复本质上是对人类数据标注员行为的 **有损模拟**。
> - 由于认知差异，模型可能会出现 **幻觉、能力上的“瑞士奶酪”现象**（某些简单任务会失败）。
> - 当你使用 **03 mini** 等“推理模型”时，情况有所不同。这些模型经过 **强化学习** 训练，发展出了类似人类思考过程的推理策略。
> - 这些模型的输出不仅仅是模仿人类标注员，而是在模拟中 **涌现** 出的新的思考功能。
> - 强化学习在 **可验证领域**（如数学和代码）表现出巨大潜力，但在 **不可验证领域**（如创意写作）的效果以及不同领域之间的知识迁移程度仍然是开放性问题。
> - 强化学习仍然是一个新兴领域，当前的“推理模型”还处于早期阶段，但未来有望在开放领域的思考和问题解决方面实现突破。
> - 总而言之，现在是 AI 领域令人兴奋的时代，LLM 可以极大地加速工作，但需要了解其局限性，并将其作为工具谨慎使用。

